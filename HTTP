# GitHub Pages

_Create a site or blog from your GitHub repositories with GitHub Pages._

## Welcome

- **Who is this for**: Beginners, students, project maintainers, small businesses.
- **What you'll learn**: How to build a GitHub Pages site.
- **What you'll build**: We'll build a simple GitHub Pages site with a blog. We'll use [Jekyll](https://jekyllrb.com), a static site generator.
- **Prerequisites**: If you need to learn about branches, commits, and pull requests, take [Introduction to GitHub](https://github.com/skills/introduction-to-github) first.

- **How long**: This exercise takes less than one hour to complete.

In this exercise, you will:

1. Enable GitHub Pages
1. Configure your site
1. Customize your home page
1. Create a blog post
1. Merge your pull request


### How to start this exercise

Simply copy the exercise to your account, then give your favorite Octocat (Mona) **about 20 seconds** to prepare the first lesson, then **refresh the page**.

[![](https://img.shields.io/badge/Copy%20Exercise-%E2%86%92-1f883d?style=for-the-badge&logo=github&labelColor=197935)](https://github.com/new?template_owner=skills&template_name=github-pages&owner=%40me&name=skills-github-pages&description=Exercise:+Create+a+site+or+blog+from+your+GitHub+repositories+with+GitHub+Pages&visibility=public)

<details>
<summary>Having trouble? ü§∑</summary><br/>

When copying the exercise, we recommend the following settings:

- For owner, choose your personal account or an organization to host the repository.

- We recommend creating a public repository, since private repositories will use Actions minutes.

If the exercise isn't ready in 20 seconds, please check the [Actions](../../actions) tab.

- Check to see if a job is running. Sometimes it simply takes a bit longer.

- If the page shows a failed job, please submit an issue. Nice, you found a bug! üêõ

</details>

---

&copy; 2025 GitHub &bull; [Code of Conduct](https://www.contributor-covenant.org/version/2/1/code_of_conduct/code_of_conduct.md) &bull; [MIT License](https://gh.io/mit)

HTTP.
Got
Sindre's open source work is supported by the community.
Special thanks to:

Human-friendly and powerful HTTP request library for Node.js

Downloads Install size

See how Got compares to other HTTP libraries

You probably want Ky instead, by the same people. It's smaller, works in the browser too, and is more stable since it's built on Fetch. Or fetch-extras for simple needs.

Support questions should be asked here.

Install
npm install got
Warning: This package is native ESM and no longer provides a CommonJS export. If your project uses CommonJS, you will have to convert to ESM. Please don't open issues for questions regarding CommonJS / ESM.

Got v11 is no longer maintained and we will not accept any backport requests.

Take a peek
A quick start guide is available.

JSON mode
Got has a dedicated option for handling JSON payload.
Furthermore, the promise exposes a .json<T>() function that returns Promise<T>.

import got from 'got';

const {data} = await got.post('https://httpbin.org/anything', {
 json: {
  hello: 'world'
 }
}).json();

console.log(data);
//=> {"hello": "world"}
For advanced JSON usage, check out the parseJson and stringifyJson options.

For more useful tips like this, visit the Tips page.

Highlights
Used by 10K+ packages and 5M+ repos
Actively maintained
Trusted by many companies
Documentation
By default, Got will retry on failure. To disable this option, set options.retry.limit to 0.

Main API
 Promise API
 Options
 Stream API
 Pagination API
 Advanced HTTPS API
 HTTP/2 support
 Response class
Timeouts and retries
 Advanced timeout handling
 Retries on failure
 Errors with metadata
Advanced creation
 Hooks
 Instances
 Progress events & other events
 Plugins
 Compose
Cache, Proxy and UNIX sockets
 RFC compliant caching
 Proxy support
 Unix Domain Sockets
Integration
 TypeScript support
 AWS
 Testing
Migration guides
Request migration guide
(Note that Request is unmaintained)
Axios
Node.js
Got plugins
got4aws - Got convenience wrapper to interact with AWS v4 signed APIs
gh-got - Got convenience wrapper to interact with the GitHub API
gl-got - Got convenience wrapper to interact with the GitLab API
gotql - Got convenience wrapper to interact with GraphQL using JSON-parsed queries instead of strings
got-fetch - Got with a fetch interface
got-scraping - Got wrapper specifically designed for web scraping purposes
got-ssrf - Got wrapper to protect server-side requests against SSRF attacks
Comparison
got node-fetch ky axios superagent
HTTP/2 support emoji symbols:heavy_check_mark¬π emoji symbols:x emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:heavy_check_mark**
Browser support emoji symbols:x emoji symbols:heavy_check_mark* emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark
Promise API emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark
Stream API emoji symbols:heavy_check_mark Node.js only emoji symbols:x emoji symbols:x emoji symbols:heavy_check_mark
Pagination API emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:x emoji symbols:x emoji symbols:x
Request cancelation emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark
RFC compliant caching emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:x emoji symbols:x emoji symbols:x
Cookies (out-of-the-box) emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:x emoji symbols:x emoji symbols:x
Follows redirects emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark
Retries on failure emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:heavy_check_mark
Progress events emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:heavy_check_mark*** Browser only emoji symbols:heavy_check_mark
Handles gzip/deflate emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark
Advanced timeouts emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:x emoji symbols:x emoji symbols:x
Timings emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:x emoji symbols:x emoji symbols:x
Errors with metadata emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:x
JSON mode emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark
Custom defaults emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:x
Composable emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:x emoji symbols:x emoji symbols:heavy_check_mark
Hooks emoji symbols:heavy_check_mark emoji symbols:x emoji symbols:heavy_check_mark emoji symbols:heavy_check_mark emoji symbols:x
Issues open     
Issues closed     
Downloads     
Coverage TBD    
Build     
Bugs     
Dependents     
Install size     
GitHub stars     
TypeScript support     
Last commit     
* It's almost API compatible with the browser fetch API.
** Need to switch the protocol manually. Doesn't accept PUSH streams and doesn't reuse HTTP/2 sessions.
*** Currently, only DownloadProgress event is supported, UploadProgress event is not supported.
¬π Requires Node.js 15.10.0 or above.
emoji symbols:sparkle Almost-stable feature, but the API may change. Don't hesitate to try it out!
emoji symbols:grey_question Feature in early stage of development. Very experimental.

Click here to see the install size of the Got dependencies.

Maintainers
Sindre Sorhus Szymon Marczak
Sindre Sorhus Szymon Marczak

These amazing companies are using Got
   
   
	

Segment is a happy user of Got! Got powers the main backend API that our app talks to. It's used by our in-house RPC client that we use to communicate with all microservices.

‚Äî Vadim Demedes

Antora, a static site generator for creating documentation sites, uses Got to download the UI bundle. In Antora, the UI bundle (aka theme) is maintained as a separate project. That project exports the UI as a zip file we call the UI bundle. The main site generator downloads that UI from a URL using Got and streams it to vinyl-zip to extract the files. Those files go on to be used to create the HTML pages and supporting assets.

‚Äî Dan Allen

GetVoIP is happily using Got in production. One of the unique capabilities of Got is the ability to handle Unix sockets which enables us to build a full control interfaces for our docker stack.

‚Äî Daniel Kalen

We're using Got inside of Exoframe to handle all the communication between CLI and server. Exoframe is a self-hosted tool that allows simple one-command deployments using Docker.

‚Äî Tim Ermilov

Karaoke Mugen uses Got to fetch content updates from its online server.

‚Äî Axel Terizaki

Renovate uses Got, gh-got and gl-got to send millions of queries per day to GitHub, GitLab, npmjs, PyPi, Packagist, Docker Hub, Terraform, CircleCI, and more.

‚Äî Rhys Arkins

Resistbot uses Got to communicate from the API frontend where all correspondence ingresses to the officials lookup database in back.

‚Äî Chris Erickson

Natural Cycles is using Got to communicate with all kinds of 3rd-party REST APIs (over 9000!).

‚Äî Kirill Groshkov

Microlink is a cloud browser as an API service that uses Got widely as the main HTTP client, serving ~22M requests a month, every time a network call needs to be performed.

‚Äî Kiko Beats

We‚Äôre using Got at Radity. Thanks for such an amazing work!

‚Äî Mirzayev Farid.


Undici.

Documentation
npm
undici
TypeScript icon, indicating that this package has built-in type declarations
7.10.0 ‚Ä¢ Public ‚Ä¢ Published 23 days ago
undici
Node CI neostandard javascript style npm version codecov

An HTTP/1.1 client, written from scratch for Node.js.

Undici means eleven in Italian. 1.1 -> 11 -> Eleven -> Undici. It is also a Stranger Things reference.

Install
npm i undici
Benchmarks
The benchmark is a simple getting data example using a 50 TCP connections with a pipelining depth of 10 running on Node 22.11.0.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tests ‚îÇ Samples ‚îÇ Result ‚îÇ Tolerance ‚îÇ Difference with slowest ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 'axios' ‚îÇ 15 ‚îÇ '5708.26 req/sec' ‚îÇ '¬± 2.91 %' ‚îÇ '-' ‚îÇ
‚îÇ 'http - no keepalive' ‚îÇ 10 ‚îÇ '5809.80 req/sec' ‚îÇ '¬± 2.30 %' ‚îÇ '+ 1.78 %' ‚îÇ
‚îÇ 'request' ‚îÇ 30 ‚îÇ '5828.80 req/sec' ‚îÇ '¬± 2.91 %' ‚îÇ '+ 2.11 %' ‚îÇ
‚îÇ 'undici - fetch' ‚îÇ 40 ‚îÇ '5903.78 req/sec' ‚îÇ '¬± 2.87 %' ‚îÇ '+ 3.43 %' ‚îÇ
‚îÇ 'node-fetch' ‚îÇ 10 ‚îÇ '5945.40 req/sec' ‚îÇ '¬± 2.13 %' ‚îÇ '+ 4.15 %' ‚îÇ
‚îÇ 'got' ‚îÇ 35 ‚îÇ '6511.45 req/sec' ‚îÇ '¬± 2.84 %' ‚îÇ '+ 14.07 %' ‚îÇ
‚îÇ 'http - keepalive' ‚îÇ 65 ‚îÇ '9193.24 req/sec' ‚îÇ '¬± 2.92 %' ‚îÇ '+ 61.05 %' ‚îÇ
‚îÇ 'superagent' ‚îÇ 35 ‚îÇ '9339.43 req/sec' ‚îÇ '¬± 2.95 %' ‚îÇ '+ 63.61 %' ‚îÇ
‚îÇ 'undici - pipeline' ‚îÇ 50 ‚îÇ '13364.62 req/sec' ‚îÇ '¬± 2.93 %' ‚îÇ '+ 134.13 %' ‚îÇ
‚îÇ 'undici - stream' ‚îÇ 95 ‚îÇ '18245.36 req/sec' ‚îÇ '¬± 2.99 %' ‚îÇ '+ 219.63 %' ‚îÇ
‚îÇ 'undici - request' ‚îÇ 50 ‚îÇ '18340.17 req/sec' ‚îÇ '¬± 2.84 %' ‚îÇ '+ 221.29 %' ‚îÇ
‚îÇ 'undici - dispatch' ‚îÇ 40 ‚îÇ '22234.42 req/sec' ‚îÇ '¬± 2.94 %' ‚îÇ '+ 289.51 %' ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Quick Start
import { request } from 'undici'

const {
  statusCode,
  headers,
  trailers,
  body
} = await request('http://localhost:3000/foo')

console.log('response received', statusCode)
console.log('headers', headers)

for await (const data of body) { console.log('data', data) }

console.log('trailers', trailers)
Body Mixins
The body mixins are the most common way to format the request/response body. Mixins include:

.arrayBuffer()
.blob()
.bytes()
.json()
.text()
[!NOTE] The body returned from undici.request does not implement .formData().

Example usage:

import { request } from 'undici'

const {
  statusCode,
  headers,
  trailers,
  body
} = await request('http://localhost:3000/foo')

console.log('response received', statusCode)
console.log('headers', headers)
console.log('data', await body.json())
console.log('trailers', trailers)
Note: Once a mixin has been called then the body cannot be reused, thus calling additional mixins on .body, e.g. .body.json(); .body.text() will result in an error TypeError: unusable being thrown and returned through the Promise rejection.

Should you need to access the body in plain-text after using a mixin, the best practice is to use the .text() mixin first and then manually parse the text to the desired format.

For more information about their behavior, please reference the body mixin from the Fetch Standard.

Common API Methods
This section documents our most commonly used API methods. Additional APIs are documented in their own files within the docs folder and are accessible via the navigation list on the left side of the docs site.

undici.request([url, options]): Promise
Arguments:

url string | URL | UrlObject
options RequestOptions
dispatcher Dispatcher - Default: getGlobalDispatcher
method String - Default: PUT if options.body, otherwise GET
Returns a promise with the result of the Dispatcher.request method.

Calls options.dispatcher.request(options).

See Dispatcher.request for more details, and request examples for examples.

undici.stream([url, options, ]factory): Promise
Arguments:

url string | URL | UrlObject
options StreamOptions
dispatcher Dispatcher - Default: getGlobalDispatcher
method String - Default: PUT if options.body, otherwise GET
factory Dispatcher.stream.factory
Returns a promise with the result of the Dispatcher.stream method.

Calls options.dispatcher.stream(options, factory).

See Dispatcher.stream for more details.

undici.pipeline([url, options, ]handler): Duplex
Arguments:

url string | URL | UrlObject
options PipelineOptions
dispatcher Dispatcher - Default: getGlobalDispatcher
method String - Default: PUT if options.body, otherwise GET
handler Dispatcher.pipeline.handler
Returns: stream.Duplex

Calls options.dispatch.pipeline(options, handler).

See Dispatcher.pipeline for more details.

undici.connect([url, options]): Promise
Starts two-way communications with the requested resource using HTTP CONNECT.

Arguments:

url string | URL | UrlObject
options ConnectOptions
dispatcher Dispatcher - Default: getGlobalDispatcher
callback (err: Error | null, data: ConnectData | null) => void (optional)
Returns a promise with the result of the Dispatcher.connect method.

Calls options.dispatch.connect(options).

See Dispatcher.connect for more details.

undici.fetch(input[, init]): Promise
Implements fetch.

https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/fetch
https://fetch.spec.whatwg.org/#fetch-method
Basic usage example:

import { fetch } from 'undici'


const res = await fetch('https://example.com')
const json = await res.json()
console.log(json)
You can pass an optional dispatcher to fetch as:

import { fetch, Agent } from 'undici'

const res = await fetch('https://example.com', {
  // Mocks are also supported
  dispatcher: new Agent({
    keepAliveTimeout: 10,
    keepAliveMaxTimeout: 10
  })
})
const json = await res.json()
console.log(json)
request.body
A body can be of the following types:

ArrayBuffer
ArrayBufferView
AsyncIterables
Blob
Iterables
String
URLSearchParams
FormData
In this implementation of fetch, request.body now accepts Async Iterables. It is not present in the Fetch Standard.

import { fetch } from 'undici'

const data = {
  async *[Symbol.asyncIterator]() {
    yield 'hello'
    yield 'world'
  },
}

await fetch('https://example.com', { body: data, method: 'POST', duplex: 'half' })
FormData besides text data and buffers can also utilize streams via Blob objects:

import { openAsBlob } from 'node:fs'

const file = await openAsBlob('./big.csv')
const body = new FormData()
body.set('file', file, 'big.csv')

await fetch('http://example.com', { method: 'POST', body })
request.duplex
'half'
In this implementation of fetch, request.duplex must be set if request.body is ReadableStream or Async Iterables, however, even though the value must be set to 'half', it is actually a full duplex. For more detail refer to the Fetch Standard.

response.body
Nodejs has two kinds of streams: web streams, which follow the API of the WHATWG web standard found in browsers, and an older Node-specific streams API. response.body returns a readable web stream. If you would prefer to work with a Node stream you can convert a web stream using .fromWeb().

import { fetch } from 'undici'
import { Readable } from 'node:stream'

const response = await fetch('https://example.com')
const readableWebStream = response.body
const readableNodeStream = Readable.fromWeb(readableWebStream)
Specification Compliance
This section documents parts of the HTTP/1.1 and Fetch Standard that Undici does not support or does not fully implement.

CORS
Unlike browsers, Undici does not implement CORS (Cross-Origin Resource Sharing) checks by default. This means:

No preflight requests are automatically sent for cross-origin requests
No validation of Access-Control-Allow-Origin headers is performed
Requests to any origin are allowed regardless of the source
This behavior is intentional for server-side environments where CORS restrictions are typically unnecessary. If your application requires CORS-like protections, you will need to implement these checks manually.

Garbage Collection
https://fetch.spec.whatwg.org/#garbage-collection
The Fetch Standard allows users to skip consuming the response body by relying on garbage collection to release connection resources. Undici does not do the same. Therefore, it is important to always either consume or cancel the response body.

Garbage collection in Node is less aggressive and deterministic (due to the lack of clear idle periods that browsers have through the rendering refresh rate) which means that leaving the release of connection resources to the garbage collector can lead to excessive connection usage, reduced performance (due to less connection re-use), and even stalls or deadlocks when running out of connections.

// Do
const { body, headers } = await fetch(url);
for await (const chunk of body) {
  // force consumption of body
}

// Do not
const { headers } = await fetch(url);
The same applies for request too:

// Do
const { body, headers } = await request(url);
await res.body.dump(); // force consumption of body

// Do not
const { headers } = await request(url);
However, if you want to get only headers, it might be better to use HEAD request method. Usage of this method will obviate the need for consumption or cancelling of the response body. See MDN - HTTP - HTTP request methods - HEAD for more details.

const headers = await fetch(url, { method: 'HEAD' })
  .then(res => res.headers)
Forbidden and Safelisted Header Names
https://fetch.spec.whatwg.org/#cors-safelisted-response-header-name
https://fetch.spec.whatwg.org/#forbidden-header-name
https://fetch.spec.whatwg.org/#forbidden-response-header-name
https://github.com/wintercg/fetch/issues/6
The Fetch Standard requires implementations to exclude certain headers from requests and responses. In browser environments, some headers are forbidden so the user agent remains in full control over them. In Undici, these constraints are removed to give more control to the user.

undici.upgrade([url, options]): Promise
Upgrade to a different protocol. See MDN - HTTP - Protocol upgrade mechanism for more details.

Arguments:

url string | URL | UrlObject
options UpgradeOptions
dispatcher Dispatcher - Default: getGlobalDispatcher
callback (error: Error | null, data: UpgradeData) => void (optional)
Returns a promise with the result of the Dispatcher.upgrade method.

Calls options.dispatcher.upgrade(options).

See Dispatcher.upgrade for more details.

undici.setGlobalDispatcher(dispatcher)
dispatcher Dispatcher
Sets the global dispatcher used by Common API Methods. Global dispatcher is shared among compatible undici modules, including undici that is bundled internally with node.js.

undici.getGlobalDispatcher()
Gets the global dispatcher used by Common API Methods.

Returns: Dispatcher

undici.setGlobalOrigin(origin)
origin string | URL | undefined
Sets the global origin used in fetch.

If undefined is passed, the global origin will be reset. This will cause Response.redirect, new Request(), and fetch to throw an error when a relative path is passed.

setGlobalOrigin('http://localhost:3000')

const response = await fetch('/api/ping')

console.log(response.url) // http://localhost:3000/api/ping
undici.getGlobalOrigin()
Gets the global origin used in fetch.

Returns: URL

UrlObject
port string | number (optional)
path string (optional)
pathname string (optional)
hostname string (optional)
origin string (optional)
protocol string (optional)
search string (optional)
Expect
Undici does not support the Expect request header field. The request body is always immediately sent and the 100 Continue response will be ignored.

Refs: https://tools.ietf.org/html/rfc7231#section-5.1.1

Pipelining
Undici will only use pipelining if configured with a pipelining factor greater than 1. Also it is important to pass blocking: false to the request options to properly pipeline requests.

Undici always assumes that connections are persistent and will immediately pipeline requests, without checking whether the connection is persistent. Hence, automatic fallback to HTTP/1.0 or HTTP/1.1 without pipelining is not supported.

Undici will immediately pipeline when retrying requests after a failed connection. However, Undici will not retry the first remaining requests in the prior pipeline and instead error the corresponding callback/promise/stream.

Undici will abort all running requests in the pipeline when any of them are aborted.

Refs: https://tools.ietf.org/html/rfc2616#section-8.1.2.2
Refs: https://tools.ietf.org/html/rfc7230#section-6.3.2
Manual Redirect
Since it is not possible to manually follow an HTTP redirect on the server-side, Undici returns the actual response instead of an opaqueredirect filtered one when invoked with a manual redirect. This aligns fetch() with the other implementations in Deno and Cloudflare Workers.

Refs: https://fetch.spec.whatwg.org/#atomic-http-redirect-handling

Workarounds
Network address family autoselection.
If you experience problem when connecting to a remote server that is resolved by your DNS servers to a IPv6 (AAAA record) first, there are chances that your local router or ISP might have problem connecting to IPv6 networks. In that case undici will throw an error with code UND_ERR_CONNECT_TIMEOUT.

If the target server resolves to both a IPv6 and IPv4 (A records) address and you are using a compatible Node version (18.3.0 and above), you can fix the problem by providing the autoSelectFamily option (support by both undici.request and undici.Agent) which will enable the family autoselection algorithm when establishing the connection.

Collaborators
Daniele Belardi, https://www.npmjs.com/~dnlup
Ethan Arrowood, https://www.npmjs.com/~ethan_arrowood
Matteo Collina, https://www.npmjs.com/~matteo.collina
Matthew Aitken, https://www.npmjs.com/~khaf
Robert Nagy, https://www.npmjs.com/~ronag
Szymon Marczak, https://www.npmjs.com/~szmarczak
Past Collaborators
Tomas Della Vedova, https://www.npmjs.com/~delvedor
Releasers
Ethan Arrowood, https://www.npmjs.com/~ethan_arrowood
Matteo Collina, https://www.npmjs.com/~matteo.collina
Robert Nagy, https://www.npmjs.com/~ronag
Matthew Aitken, https://www.npmjs.com/~khaf
Long Term Support
Undici aligns with the Node.js LTS schedule. The following table shows the supported versions:

Version Node.js End of Life
5.x v18.x 2024-04-30
6.x v20.x v22.x 2026-04-30
7.x v24.x 2027-04-30
License
MIT

Readme
Keywords
fetchhttphttpspromiserequestcurlwgetxhrwhatwg
Provenance
Built and signed on
GitHub Actions
View build summary
Source Commit

github.com/nodejs/undici@5ad8998
Build File

.github/workflows/release.yml
Public Ledger

Transparency log entry
Share feedback
Package Sidebar.
Install 
npm i undici

Repository
github.com/nodejs/undici

Homepage
undici.nodejs.org.

Version 7.10.0 . License MIT.


Ky-Unirvesal.

ky-universal
Use Ky in both Node.js and browsers

As of Ky 1.0.0, it runs natively on Node.js. So this package is no longer needed.

Ky is made for browsers, but this package makes it possible to use it in Node.js too, by polyfilling most of the required browser APIs using node-fetch.

This package can be useful for:

Isomorphic code
Web apps (React, Vue.js, etc.) that use server-side rendering (SSR)
Testing browser libraries using a Node.js test runner
Note: Before opening an issue, make sure it's an issue with Ky and not its polyfills. Generally, if something works in the browser, but not in Node.js, it's an issue with node-fetch.

Keep in mind that Ky targets modern browsers when used in the browser. For older browsers, you will need to transpile and use a fetch polyfill.

Install
npm install ky ky-universal
Note that you also need to install ky.

Usage
import ky from 'ky-universal';

const parsed = await ky('https://httpbin.org/json').json();

// ‚Ä¶
ReadableStream support
For ReadableStream support, also install web-streams-polyfill:

$ npm install web-streams-polyfill
You can then use it normally:

import ky from 'ky-universal';

const {body} = await ky('https://httpbin.org/bytes/16');
const {value} = await body.getReader().read();
const result = new TextDecoder('utf-8').decode(value);

// ‚Ä¶
API
The API is exactly the same as the Ky API, including the named exports.

web app (React, Vue.js, etc.) that uses server-side rendering (SSR).
Use it like you would use Ky:

import ky from 'ky-universal';

const parsed = await ky('https://httpbin.org/json').json();

// ‚Ä¶
Webpack will ensure the polyfills are only included and used when the app is rendered on the server-side.

test a browser library that uses Ky in AVA.
Put the following in package.json:

{
 "ava": {
  "require": [
   "ky-universal"
  ]
 }
}
The library that uses Ky will now just work in AVA tests.

clone() hangs with a large response in Node - What should I do?
Streams in Node.js have a smaller internal buffer size (16 kB, aka highWaterMark) than browsers (>1 MB, not consistent across browsers). When using Ky, the default highWaterMark is set to 10 MB, so you shouldn't encounter many issues related to that.

However, you can specify a custom highWaterMark if needed:

import ky from 'ky-universal';

const response = await ky('https://example.com', {
 // 20 MB
 highWaterMark: 1000 * 1000 * 20
});

const data = await response.clone().buffer();
Related
ky - Tiny and elegant HTTP client based on the browser Fetch API
got - Simplified HTTP requests in Node.js


Node-fetch.

Node Fetch
A light-weight module that brings Fetch API to Node.js.

Build status Coverage status Current version Install size Mentioned in Awesome Node.js Discord

Consider supporting us on our Open Collective:

Open Collective
You might be looking for the v2 docs

Motivation
Features
Difference from client-side fetch
Installation
Loading and configuring the module
Upgrading
Common Usage
Plain text or HTML
JSON
Simple Post
Post with JSON
Post with form parameters
Handling exceptions
Handling client and server errors
Handling cookies
Advanced Usage
Streams
Accessing Headers and other Metadata
Extract Set-Cookie Header
Post data using a file
Request cancellation with AbortSignal
API
fetch(url[, options])
Options
Default Headers
Custom Agent
Custom highWaterMark
Insecure HTTP Parser
Class: Request
new Request(input[, options])
Class: Response
new Response([body[, options]])
response.ok
response.redirected
response.type
Class: Headers
new Headers([init])
Interface: Body
body.body
body.bodyUsed
body.arrayBuffer()
body.blob()
body.formData()
body.json()
body.text()
Class: FetchError
Class: AbortError
TypeScript
Acknowledgement
Team - Former
License
Motivation
Instead of implementing XMLHttpRequest in Node.js to run browser-specific Fetch polyfill, why not go from native http to fetch API directly? Hence, node-fetch, minimal code for a window.fetch compatible API on Node.js runtime.

See Jason Miller's isomorphic-unfetch or Leonardo Quixada's cross-fetch for isomorphic usage (exports node-fetch for server-side, whatwg-fetch for client-side).

Features
Stay consistent with window.fetch API.
Make conscious trade-off when following WHATWG fetch spec and stream spec implementation details, document known differences.
Use native promise and async functions.
Use native Node streams for body, on both request and response.
Decode content encoding (gzip/deflate/brotli) properly, and convert string output (such as res.text() and res.json()) to UTF-8 automatically.
Useful extensions such as redirect limit, response size limit, explicit errors for troubleshooting.
Difference from client-side fetch
See known differences:
As of v3.x
As of v2.x
If you happen to use a missing feature that window.fetch offers, feel free to open an issue.
Pull requests are welcomed too!
Installation
Current stable release (3.x) requires at least Node.js 12.20.0.

npm install node-fetch
Loading and configuring the module
ES Modules (ESM)
import fetch from 'node-fetch';
CommonJS
node-fetch from v3 is an ESM-only module - you are not able to import it with require().

If you cannot switch to ESM, please use v2 which remains compatible with CommonJS. Critical bug fixes will continue to be published for v2.

npm install node-fetch@2
Alternatively, you can use the async import() function from CommonJS to load node-fetch asynchronously:

// mod.cjs
const fetch = (...args) => import('node-fetch').then(({default: fetch}) => fetch(...args));
Providing global access
To use fetch() without importing it, you can patch the global object in node:

// fetch-polyfill.js
import fetch, {
  Blob,
  blobFrom,
  blobFromSync,
  File,
  fileFrom,
  fileFromSync,
  FormData,
  Headers,
  Request,
  Response,
} from 'node-fetch'

if (!globalThis.fetch) {
  globalThis.fetch = fetch
  globalThis.Headers = Headers
  globalThis.Request = Request
  globalThis.Response = Response
}

// index.js
import './fetch-polyfill'

// ...
Upgrading
Using an old version of node-fetch? Check out the following files:

2.x to 3.x upgrade guide
1.x to 2.x upgrade guide
Changelog
Common Usage
NOTE: The documentation below is up-to-date with 3.x releases, if you are using an older version, please check how to upgrade.

Plain text or HTML
import fetch from 'node-fetch';

const response = await fetch('https://github.com/');
const body = await response.text();

console.log(body);
JSON
import fetch from 'node-fetch';

const response = await fetch('https://api.github.com/users/github');
const data = await response.json();

console.log(data);
Simple Post
import fetch from 'node-fetch';

const response = await fetch('https://httpbin.org/post', {method: 'POST', body: 'a=1'});
const data = await response.json();

console.log(data);
Post with JSON
import fetch from 'node-fetch';

const body = {a: 1};

const response = await fetch('https://httpbin.org/post', {
 method: 'post',
 body: JSON.stringify(body),
 headers: {'Content-Type': 'application/json'}
});
const data = await response.json();

console.log(data);
Post with form parameters
URLSearchParams is available on the global object in Node.js as of v10.0.0. See official documentation for more usage methods.

NOTE: The Content-Type header is only set automatically to x-www-form-urlencoded when an instance of URLSearchParams is given as such:

import fetch from 'node-fetch';

const params = new URLSearchParams();
params.append('a', 1);

const response = await fetch('https://httpbin.org/post', {method: 'POST', body: params});
const data = await response.json();

console.log(data);
Handling exceptions
NOTE: 3xx-5xx responses are NOT exceptions, and should be handled in then(), see the next section.

Wrapping the fetch function into a try/catch block will catch all exceptions, such as errors originating from node core libraries, like network errors, and operational errors which are instances of FetchError. See the error handling document for more details.

import fetch from 'node-fetch';

try {
 await fetch('https://domain.invalid/');
} catch (error) {
 console.log(error);
}
Handling client and server errors
It is common to create a helper function to check that the response contains no client (4xx) or server (5xx) error responses:

import fetch from 'node-fetch';

class HTTPResponseError extends Error {
 constructor(response) {
  super(`HTTP Error Response: ${response.status} ${response.statusText}`);
  this.response = response;
 }
}

const checkStatus = response => {
 if (response.ok) {
  // response.status >= 200 && response.status < 300
  return response;
 } else {
  throw new HTTPResponseError(response);
 }
}

const response = await fetch('https://httpbin.org/status/400');

try {
 checkStatus(response);
} catch (error) {
 console.error(error);

 const errorBody = await error.response.text();
 console.error(`Error body: ${errorBody}`);
}
Handling cookies
Cookies are not stored by default. However, cookies can be extracted and passed by manipulating request and response headers. See Extract Set-Cookie Header for details.

Advanced Usage
Streams
The "Node.js way" is to use streams when possible. You can pipe res.body to another stream. This example uses stream.pipeline to attach stream error handlers and wait for the download to complete.

import {createWriteStream} from 'node:fs';
import {pipeline} from 'node:stream';
import {promisify} from 'node:util'
import fetch from 'node-fetch';

const streamPipeline = promisify(pipeline);

const response = await fetch('https://github.githubassets.com/images/modules/logos_page/Octocat.png');

if (!response.ok) throw new Error(`unexpected response ${response.statusText}`);

await streamPipeline(response.body, createWriteStream('./octocat.png'));
In Node.js 14 you can also use async iterators to read body; however, be careful to catch errors -- the longer a response runs, the more likely it is to encounter an error.

import fetch from 'node-fetch';

const response = await fetch('https://httpbin.org/stream/3');

try {
 for await (const chunk of response.body) {
  console.dir(JSON.parse(chunk.toString()));
 }
} catch (err) {
 console.error(err.stack);
}
In Node.js 12 you can also use async iterators to read body; however, async iterators with streams did not mature until Node.js 14, so you need to do some extra work to ensure you handle errors directly from the stream and wait on it response to fully close.

import fetch from 'node-fetch';

const read = async body => {
 let error;
 body.on('error', err => {
  error = err;
 });

 for await (const chunk of body) {
  console.dir(JSON.parse(chunk.toString()));
 }

 return new Promise((resolve, reject) => {
  body.on('close', () => {
   error ? reject(error) : resolve();
  });
 });
};

try {
 const response = await fetch('https://httpbin.org/stream/3');
 await read(response.body);
} catch (err) {
 console.error(err.stack);
}
Accessing Headers and other Metadata
import fetch from 'node-fetch';

const response = await fetch('https://github.com/');

console.log(response.ok);
console.log(response.status);
console.log(response.statusText);
console.log(response.headers.raw());
console.log(response.headers.get('content-type'));
Extract Set-Cookie Header
Unlike browsers, you can access raw Set-Cookie headers manually using Headers.raw(). This is a node-fetch only API.

import fetch from 'node-fetch';

const response = await fetch('https://example.com');

// Returns an array of values, instead of a string of comma-separated values
console.log(response.headers.raw()['set-cookie']);
Post data using a file
import fetch, {
  Blob,
  blobFrom,
  blobFromSync,
  File,
  fileFrom,
  fileFromSync,
} from 'node-fetch'

const mimetype = 'text/plain'
const blob = fileFromSync('./input.txt', mimetype)
const url = 'https://httpbin.org/post'

const response = await fetch(url, { method: 'POST', body: blob })
const data = await response.json()

console.log(data)
node-fetch comes with a spec-compliant FormData implementations for posting multipart/form-data payloads

import fetch, { FormData, File, fileFrom } from 'node-fetch'

const httpbin = 'https://httpbin.org/post'
const formData = new FormData()
const binary = new Uint8Array([ 97, 98, 99 ])
const abc = new File([binary], 'abc.txt', { type: 'text/plain' })

formData.set('greeting', 'Hello, world!')
formData.set('file-upload', abc, 'new name.txt')

const response = await fetch(httpbin, { method: 'POST', body: formData })
const data = await response.json()

console.log(data)
If you for some reason need to post a stream coming from any arbitrary place, then you can append a Blob or a File look-a-like item.

The minimum requirement is that it has:

A Symbol.toStringTag getter or property that is either Blob or File
A known size.
And either a stream() method or a arrayBuffer() method that returns a ArrayBuffer.
The stream() must return any async iterable object as long as it yields Uint8Array (or Buffer) so Node.Readable streams and whatwg streams works just fine.

formData.append('upload', {
 [Symbol.toStringTag]: 'Blob',
 size: 3,
  *stream() {
    yield new Uint8Array([97, 98, 99])
 },
 arrayBuffer() {
  return new Uint8Array([97, 98, 99]).buffer
 }
}, 'abc.txt')
Request cancellation with AbortSignal
You may cancel requests with AbortController. A suggested implementation is abort-controller.

An example of timing out a request after 150ms could be achieved as the following:

import fetch, { AbortError } from 'node-fetch';

// AbortController was added in node v14.17.0 globally
const AbortController = globalThis.AbortController || await import('abort-controller')

const controller = new AbortController();
const timeout = setTimeout(() => {
 controller.abort();
}, 150);

try {
 const response = await fetch('https://example.com', {signal: controller.signal});
 const data = await response.json();
} catch (error) {
 if (error instanceof AbortError) {
  console.log('request was aborted');
 }
} finally {
 clearTimeout(timeout);
}
See test cases for more examples.

API
fetch(url[, options])
url A string representing the URL for fetching
options Options for the HTTP(S) request
Returns: Promise<Response>
Perform an HTTP(S) fetch.

url should be an absolute URL, such as https://example.com/. A path-relative URL (/file/under/root) or protocol-relative URL (//can-be-http-or-https.com/) will result in a rejected Promise.


Options
The default values are shown after each option key.

{
 // These properties are part of the Fetch Standard
 method: 'GET',
 headers: {}, // Request headers. format is the identical to that accepted by the Headers constructor (see below)
 body: null, // Request body. can be null, or a Node.js Readable stream
 redirect: 'follow', // Set to `manual` to extract redirect headers, `error` to reject redirect
 signal: null, // Pass an instance of AbortSignal to optionally abort requests

 // The following properties are node-fetch extensions
 follow: 20, // maximum redirect count. 0 to not follow redirect
 compress: true, // support gzip/deflate content encoding. false to disable
 size: 0, // maximum response body size in bytes. 0 to disable
 agent: null, // http(s).Agent instance or function that returns an instance (see below)
 highWaterMark: 16384, // the maximum number of bytes to store in the internal buffer before ceasing to read from the underlying resource.
 insecureHTTPParser: false // Use an insecure HTTP parser that accepts invalid HTTP headers when `true`.
}
Default Headers
If no values are set, the following request headers will be sent automatically:

Header Value
Accept-Encoding gzip, deflate, br (when options.compress === true)
Accept */*
Content-Length (automatically calculated, if possible)
Host (host and port information from the target URI)
Transfer-Encoding chunked (when req.body is a stream)
User-Agent node-fetch
Note: when body is a Stream, Content-Length is not set automatically.

Custom Agent
The agent option allows you to specify networking related options which are out of the scope of Fetch, including and not limited to the following:

Support self-signed certificate
Use only IPv4 or IPv6
Custom DNS Lookup
See http.Agent for more information.

If no agent is specified, the default agent provided by Node.js is used. Note that this changed in Node.js 19 to have keepalive true by default. If you wish to enable keepalive in an earlier version of Node.js, you can override the agent as per the following code sample.

In addition, the agent option accepts a function that returns http(s).Agent instance given current URL, this is useful during a redirection chain across HTTP and HTTPS protocol.

import http from 'node:http';
import https from 'node:https';

const httpAgent = new http.Agent({
 keepAlive: true
});
const httpsAgent = new https.Agent({
 keepAlive: true
});

const options = {
 agent: function(_parsedURL) {
  if (_parsedURL.protocol == 'http:') {
   return httpAgent;
  } else {
   return httpsAgent;
  }
 }
};

Custom highWaterMark
Stream on Node.js have a smaller internal buffer size (16kB, aka highWaterMark) from client-side browsers (>1MB, not consistent across browsers). Because of that, when you are writing an isomorphic app and using res.clone(), it will hang with large response in Node.

The recommended way to fix this problem is to resolve cloned response in parallel:

import fetch from 'node-fetch';

const response = await fetch('https://example.com');
const r1 = response.clone();

const results = await Promise.all([response.json(), r1.text()]);

console.log(results[0]);
console.log(results[1]);
If for some reason you don't like the solution above, since 3.x you are able to modify the highWaterMark option:

import fetch from 'node-fetch';

const response = await fetch('https://example.com', {
 // About 1MB
 highWaterMark: 1024 * 1024
});

const result = await res.clone().arrayBuffer();
console.dir(result);
Insecure HTTP Parser
Passed through to the insecureHTTPParser option on http(s).request. See http.request for more information.

Manual Redirect
The redirect: 'manual' option for node-fetch is different from the browser & specification, which results in an opaque-redirect filtered response. node-fetch gives you the typical basic filtered response instead.

import fetch from 'node-fetch';

const response = await fetch('https://httpbin.org/status/301', { redirect: 'manual' });

if (response.status === 301 || response.status === 302) {
 const locationURL = new URL(response.headers.get('location'), response.url);
 const response2 = await fetch(locationURL, { redirect: 'manual' });
 console.dir(response2);
}

Class: Request
An HTTP(S) request containing information about URL, method, headers, and the body. This class implements the Body interface.

Due to the nature of Node.js, the following properties are not implemented at this moment:

type
destination
mode
credentials
cache
integrity
keepalive
The following node-fetch extension properties are provided:

follow
compress
counter
agent
highWaterMark
See options for exact meaning of these extensions.

new Request(input[, options])
(spec-compliant)

input A string representing a URL, or another Request (which will be cloned)
options Options for the HTTP(S) request
Constructs a new Request object. The constructor is identical to that in the browser.

In most cases, directly fetch(url, options) is simpler than creating a Request object.


Class: Response
An HTTP(S) response. This class implements the Body interface.

The following properties are not implemented in node-fetch at this moment:

trailer
new Response([body[, options]])
(spec-compliant)

body A String or Readable stream
options A ResponseInit options dictionary
Constructs a new Response object. The constructor is identical to that in the browser.

Because Node.js does not implement service workers (for which this class was designed), one rarely has to construct a Response directly.

response.ok
(spec-compliant)

Convenience property representing if the request ended normally. Will evaluate to true if the response status was greater than or equal to 200 but smaller than 300.

response.redirected
(spec-compliant)

Convenience property representing if the request has been redirected at least once. Will evaluate to true if the internal redirect counter is greater than 0.

response.type
(deviation from spec)

Convenience property representing the response's type. node-fetch only supports 'default' and 'error' and does not make use of filtered responses.


Class: Headers
This class allows manipulating and iterating over a set of HTTP headers. All methods specified in the Fetch Standard are implemented.

new Headers([init])
(spec-compliant)

init Optional argument to pre-fill the Headers object
Construct a new Headers object. init can be either null, a Headers object, an key-value map object or any iterable object.

// Example adapted from https://fetch.spec.whatwg.org/#example-headers-class
import {Headers} from 'node-fetch';

const meta = {
 'Content-Type': 'text/xml'
};
const headers = new Headers(meta);

// The above is equivalent to
const meta = [['Content-Type', 'text/xml']];
const headers = new Headers(meta);

// You can in fact use any iterable objects, like a Map or even another Headers
const meta = new Map();
meta.set('Content-Type', 'text/xml');
const headers = new Headers(meta);
const copyOfHeaders = new Headers(headers);

Interface: Body
Body is an abstract interface with methods that are applicable to both Request and Response classes.

body.body
(deviation from spec)

Node.js Readable stream
Data are encapsulated in the Body object. Note that while the Fetch Standard requires the property to always be a WHATWG ReadableStream, in node-fetch it is a Node.js Readable stream.

body.bodyUsed
(spec-compliant)

Boolean
A boolean property for if this body has been consumed. Per the specs, a consumed body cannot be used again.

body.arrayBuffer()
body.formData()
body.blob()
body.json()
body.text()
fetch comes with methods to parse multipart/form-data payloads as well as x-www-form-urlencoded bodies using .formData() this comes from the idea that Service Worker can intercept such messages before it's sent to the server to alter them. This is useful for anybody building a server so you can use it to parse & consume payloads.

Code example

Class: FetchError
(node-fetch extension)

An operational error in the fetching process. See ERROR-HANDLING.md for more info.

Class: AbortError
(node-fetch extension)

An Error thrown when the request is aborted in response to an AbortSignal's abort event. It has a name property of AbortError. See ERROR-HANDLING.MD for more info.

TypeScript
Since 3.x types are bundled with node-fetch, so you don't need to install any additional packages.

For older versions please use the type definitions from DefinitelyTyped:

npm install --save-dev @types/node-fetch@2.x.

github/fetch for providing a solid implementation reference.
License MIT.


Axios.

axios
npm version CDNJS Build status Gitpod Ready-to-Code code coverage install size npm downloads gitter chat code helpers Known Vulnerabilities npm bundle size

Promise based HTTP client for the browser and node.js

New axios docs website: click here

Table of Contents
Features
Browser Support
Installing
Example
Axios API
Request method aliases
Concurrency üëé
Creating an instance
Instance methods
Request Config
Response Schema
Config Defaults
Global axios defaults
Custom instance defaults
Config order of precedence
Interceptors
Multiple Interceptors
Handling Errors
Cancellation
AbortController
CancelToken üëé
Using application/x-www-form-urlencoded format
URLSearchParams
Query string
üÜï Automatic serialization
Using multipart/form-data format
FormData
üÜï Automatic serialization
Files Posting
HTML Form Posting
Semver
Promises
TypeScript
Resources
Credits
License
Features
Make XMLHttpRequests from the browser
Make http requests from node.js
Supports the Promise API
Intercept request and response
Transform request and response data
Cancel requests
Automatic transforms for JSON data
üÜï Automatic data object serialization to multipart/form-data and x-www-form-urlencoded body encodings
Client side support for protecting against XSRF
Browser Support
Chrome Firefox Safari Opera Edge IE
Latest ‚úî Latest ‚úî Latest ‚úî Latest ‚úî Latest ‚úî 11 ‚úî
Browser Matrix

Installing
Using npm:

$ npm install axios
Using bower:

$ bower install axios
Using yarn:

$ yarn add axios
Using pnpm:

$ pnpm add axios
Using jsDelivr CDN:

<script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>
Using unpkg CDN:

<script src="https://unpkg.com/axios/dist/axios.min.js"></script>
Example
note: CommonJS usage
In order to gain the TypeScript typings (for intellisense / autocomplete) while using CommonJS imports with require() use the following approach:

const axios = require('axios').default;

// axios.<method> will now provide autocomplete and parameter typings
Performing a GET request

const axios = require('axios').default;

// Make a request for a user with a given ID
axios.get('/user?ID=12345')
  .then(function (response) {
    // handle success
    console.log(response);
  })
  .catch(function (error) {
    // handle error
    console.log(error);
  })
  .then(function () {
    // always executed
  });

// Optionally the request above could also be done as
axios.get('/user', {
    params: {
      ID: 12345
    }
  })
  .then(function (response) {
    console.log(response);
  })
  .catch(function (error) {
    console.log(error);
  })
  .then(function () {
    // always executed
  });  

// Want to use async/await? Add the `async` keyword to your outer function/method.
async function getUser() {
  try {
    const response = await axios.get('/user?ID=12345');
    console.log(response);
  } catch (error) {
    console.error(error);
  }
}
NOTE: async/await is part of ECMAScript 2017 and is not supported in Internet Explorer and older browsers, so use with caution.

Performing a POST request

axios.post('/user', {
    firstName: 'Fred',
    lastName: 'Flintstone'
  })
  .then(function (response) {
    console.log(response);
  })
  .catch(function (error) {
    console.log(error);
  });
Performing multiple concurrent requests

function getUserAccount() {
  return axios.get('/user/12345');
}

function getUserPermissions() {
  return axios.get('/user/12345/permissions');
}

Promise.all([getUserAccount(), getUserPermissions()])
  .then(function (results) {
    const acct = results[0];
    const perm = results[1];
  });
axios API
Requests can be made by passing the relevant config to axios.

axios(config)
// Send a POST request
axios({
  method: 'post',
  url: '/user/12345',
  data: {
    firstName: 'Fred',
    lastName: 'Flintstone'
  }
});
// GET request for remote image in node.js
axios({
  method: 'get',
  url: 'https://bit.ly/2mTM3nY',
  responseType: 'stream'
})
  .then(function (response) {
    response.data.pipe(fs.createWriteStream('ada_lovelace.jpg'))
  });
axios(url[, config])
// Send a GET request (default method)
axios('/user/12345');
Request method aliases
For convenience, aliases have been provided for all common request methods.

axios.request(config)
axios.get(url[, config])
axios.delete(url[, config])
axios.head(url[, config])
axios.options(url[, config])
axios.post(url[, data[, config]])
axios.put(url[, data[, config]])
axios.patch(url[, data[, config]])
NOTE
When using the alias methods url, method, and data properties don't need to be specified in config.

Concurrency (Deprecated)
Please use Promise.all to replace the below functions.

Helper functions for dealing with concurrent requests.

axios.all(iterable) axios.spread(callback)

Creating an instance
You can create a new instance of axios with a custom config.

axios.create([config])
const instance = axios.create({
  baseURL: 'https://some-domain.com/api/',
  timeout: 1000,
  headers: {'X-Custom-Header': 'foobar'}
});
Instance methods
The available instance methods are listed below. The specified config will be merged with the instance config.

axios#request(config)
axios#get(url[, config])
axios#delete(url[, config])
axios#head(url[, config])
axios#options(url[, config])
axios#post(url[, data[, config]])
axios#put(url[, data[, config]])
axios#patch(url[, data[, config]])
axios#getUri([config])
Request Config
These are the available config options for making requests. Only the url is required. Requests will default to GET if method is not specified.

{
  // `url` is the server URL that will be used for the request
  url: '/user',

  // `method` is the request method to be used when making the request
  method: 'get', // default

  // `baseURL` will be prepended to `url` unless `url` is absolute.
  // It can be convenient to set `baseURL` for an instance of axios to pass relative URLs
  // to methods of that instance.
  baseURL: 'https://some-domain.com/api/',

  // `transformRequest` allows changes to the request data before it is sent to the server
  // This is only applicable for request methods 'PUT', 'POST', 'PATCH' and 'DELETE'
  // The last function in the array must return a string or an instance of Buffer, ArrayBuffer,
  // FormData or Stream
  // You may modify the headers object.
  transformRequest: [function (data, headers) {
    // Do whatever you want to transform the data

    return data;
  }],

  // `transformResponse` allows changes to the response data to be made before
  // it is passed to then/catch
  transformResponse: [function (data) {
    // Do whatever you want to transform the data

    return data;
  }],

  // `headers` are custom headers to be sent
  headers: {'X-Requested-With': 'XMLHttpRequest'},

  // `params` are the URL parameters to be sent with the request
  // Must be a plain object or a URLSearchParams object
  params: {
    ID: 12345
  },

  // `paramsSerializer` is an optional config in charge of serializing `params`
  paramsSerializer: {
    indexes: null // array indexes format (null - no brackets, false - empty brackets, true - brackets with indexes)
  },

  // `data` is the data to be sent as the request body
  // Only applicable for request methods 'PUT', 'POST', 'DELETE , and 'PATCH'
  // When no `transformRequest` is set, must be of one of the following types:
  // - string, plain object, ArrayBuffer, ArrayBufferView, URLSearchParams
  // - Browser only: FormData, File, Blob
  // - Node only: Stream, Buffer
  data: {
    firstName: 'Fred'
  },
  
  // syntax alternative to send data into the body
  // method post
  // only the value is sent, not the key
  data: 'Country=Brasil&City=Belo Horizonte',

  // `timeout` specifies the number of milliseconds before the request times out.
  // If the request takes longer than `timeout`, the request will be aborted.
  timeout: 1000, // default is `0` (no timeout)

  // `withCredentials` indicates whether or not cross-site Access-Control requests
  // should be made using credentials
  withCredentials: false, // default

  // `adapter` allows custom handling of requests which makes testing easier.
  // Return a promise and supply a valid response (see lib/adapters/README.md).
  adapter: function (config) {
    /* ... */
  },

  // `auth` indicates that HTTP Basic auth should be used, and supplies credentials.
  // This will set an `Authorization` header, overwriting any existing
  // `Authorization` custom headers you have set using `headers`.
  // Please note that only HTTP Basic auth is configurable through this parameter.
  // For Bearer tokens and such, use `Authorization` custom headers instead.
  auth: {
    username: 'janedoe',
    password: 's00pers3cret'
  },

  // `responseType` indicates the type of data that the server will respond with
  // options are: 'arraybuffer', 'document', 'json', 'text', 'stream'
  // browser only: 'blob'
  responseType: 'json', // default

  // `responseEncoding` indicates encoding to use for decoding responses (Node.js only)
  // Note: Ignored for `responseType` of 'stream' or client-side requests
  responseEncoding: 'utf8', // default

  // `xsrfCookieName` is the name of the cookie to use as a value for xsrf token
  xsrfCookieName: 'XSRF-TOKEN', // default

  // `xsrfHeaderName` is the name of the http header that carries the xsrf token value
  xsrfHeaderName: 'X-XSRF-TOKEN', // default

  // `onUploadProgress` allows handling of progress events for uploads
  // browser only
  onUploadProgress: function (progressEvent) {
    // Do whatever you want with the native progress event
  },

  // `onDownloadProgress` allows handling of progress events for downloads
  // browser only
  onDownloadProgress: function (progressEvent) {
    // Do whatever you want with the native progress event
  },

  // `maxContentLength` defines the max size of the http response content in bytes allowed in node.js
  maxContentLength: 2000,

  // `maxBodyLength` (Node only option) defines the max size of the http request content in bytes allowed
  maxBodyLength: 2000,

  // `validateStatus` defines whether to resolve or reject the promise for a given
  // HTTP response status code. If `validateStatus` returns `true` (or is set to `null`
  // or `undefined`), the promise will be resolved; otherwise, the promise will be
  // rejected.
  validateStatus: function (status) {
    return status >= 200 && status < 300; // default
  },

  // `maxRedirects` defines the maximum number of redirects to follow in node.js.
  // If set to 0, no redirects will be followed.
  maxRedirects: 21, // default

  // `beforeRedirect` defines a function that will be called before redirect.
  // Use this to adjust the request options upon redirecting,
  // to inspect the latest response headers,
  // or to cancel the request by throwing an error
  // If maxRedirects is set to 0, `beforeRedirect` is not used.
  beforeRedirect: (options, { headers }) => {
    if (options.hostname === "example.com") {
      options.auth = "user:password";
    }
  },

  // `socketPath` defines a UNIX Socket to be used in node.js.
  // e.g. '/var/run/docker.sock' to send requests to the docker daemon.
  // Only either `socketPath` or `proxy` can be specified.
  // If both are specified, `socketPath` is used.
  socketPath: null, // default

  // `httpAgent` and `httpsAgent` define a custom agent to be used when performing http
  // and https requests, respectively, in node.js. This allows options to be added like
  // `keepAlive` that are not enabled by default.
  httpAgent: new http.Agent({ keepAlive: true }),
  httpsAgent: new https.Agent({ keepAlive: true }),

  // `proxy` defines the hostname, port, and protocol of the proxy server.
  // You can also define your proxy using the conventional `http_proxy` and
  // `https_proxy` environment variables. If you are using environment variables
  // for your proxy configuration, you can also define a `no_proxy` environment
  // variable as a comma-separated list of domains that should not be proxied.
  // Use `false` to disable proxies, ignoring environment variables.
  // `auth` indicates that HTTP Basic auth should be used to connect to the proxy, and
  // supplies credentials.
  // This will set an `Proxy-Authorization` header, overwriting any existing
  // `Proxy-Authorization` custom headers you have set using `headers`.
  // If the proxy server uses HTTPS, then you must set the protocol to `https`. 
  proxy: {
    protocol: 'https',
    host: '127.0.0.1',
    port: 9000,
    auth: {
      username: 'mikeymike',
      password: 'rapunz3l'
    }
  },

  // `cancelToken` specifies a cancel token that can be used to cancel the request
  // (see Cancellation section below for details)
  cancelToken: new CancelToken(function (cancel) {
  }),

  // an alternative way to cancel Axios requests using AbortController
  signal: new AbortController().signal,

  // `decompress` indicates whether or not the response body should be decompressed 
  // automatically. If set to `true` will also remove the 'content-encoding' header 
  // from the responses objects of all decompressed responses
  // - Node only (XHR cannot turn off decompression)
  decompress: true // default

  // `insecureHTTPParser` boolean.
  // Indicates where to use an insecure HTTP parser that accepts invalid HTTP headers.
  // This may allow interoperability with non-conformant HTTP implementations.
  // Using the insecure parser should be avoided.
  // see options https://nodejs.org/dist/latest-v12.x/docs/api/http.html#http_http_request_url_options_callback
  // see also https://nodejs.org/en/blog/vulnerability/february-2020-security-releases/#strict-http-header-parsing-none
  insecureHTTPParser: undefined // default

  // transitional options for backward compatibility that may be removed in the newer versions
  transitional: {
    // silent JSON parsing mode
    // `true` - ignore JSON parsing errors and set response.data to null if parsing failed (old behaviour)
    // `false` - throw SyntaxError if JSON parsing failed (Note: responseType must be set to 'json')
    silentJSONParsing: true, // default value for the current Axios version

    // try to parse the response string as JSON even if `responseType` is not 'json'
    forcedJSONParsing: true,
    
    // throw ETIMEDOUT error instead of generic ECONNABORTED on request timeouts
    clarifyTimeoutError: false,
  },

  env: {
    // The FormData class to be used to automatically serialize the payload into a FormData object
    FormData: window?.FormData || global?.FormData
  },

  formSerializer: {
      visitor: (value, key, path, helpers)=> {}; // custom visitor funaction to serrialize form values
      dots: boolean; // use dots instead of brackets format
      metaTokens: boolean; // keep special endings like {} in parameter key 
      indexes: boolean; // array indexes format null - no brackets, false - empty brackets, true - brackets with indexes
  }
}
Response Schema
The response for a request contains the following information.

{
  // `data` is the response that was provided by the server
  data: {},

  // `status` is the HTTP status code from the server response
  status: 200,

  // `statusText` is the HTTP status message from the server response
  statusText: 'OK',

  // `headers` the HTTP headers that the server responded with
  // All header names are lowercase and can be accessed using the bracket notation.
  // Example: `response.headers['content-type']`
  headers: {},

  // `config` is the config that was provided to `axios` for the request
  config: {},

  // `request` is the request that generated this response
  // It is the last ClientRequest instance in node.js (in redirects)
  // and an XMLHttpRequest instance in the browser
  request: {}
}
When using then, you will receive the response as follows:

axios.get('/user/12345')
  .then(function (response) {
    console.log(response.data);
    console.log(response.status);
    console.log(response.statusText);
    console.log(response.headers);
    console.log(response.config);
  });
When using catch, or passing a rejection callback as second parameter of then, the response will be available through the error object as explained in the Handling Errors section.

Config Defaults
You can specify config defaults that will be applied to every request.

Global axios defaults
axios.defaults.baseURL = 'https://api.example.com';

// Important: If axios is used with multiple domains, the AUTH_TOKEN will be sent to all of them.
// See below for an example using Custom instance defaults instead.
axios.defaults.headers.common['Authorization'] = AUTH_TOKEN;

axios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded';
Custom instance defaults
// Set config defaults when creating the instance
const instance = axios.create({
  baseURL: 'https://api.example.com'
});

// Alter defaults after instance has been created
instance.defaults.headers.common['Authorization'] = AUTH_TOKEN;
Config order of precedence
Config will be merged with an order of precedence. The order is library defaults found in lib/defaults.js, then defaults property of the instance, and finally config argument for the request. The latter will take precedence over the former. Here's an example.

// Create an instance using the config defaults provided by the library
// At this point the timeout config value is `0` as is the default for the library
const instance = axios.create();

// Override timeout default for the library
// Now all requests using this instance will wait 2.5 seconds before timing out
instance.defaults.timeout = 2500;

// Override timeout for this request as it's known to take a long time
instance.get('/longRequest', {
  timeout: 5000
});
Interceptors
You can intercept requests or responses before they are handled by then or catch.

// Add a request interceptor
axios.interceptors.request.use(function (config) {
    // Do something before request is sent
    return config;
  }, function (error) {
    // Do something with request error
    return Promise.reject(error);
  });

// Add a response interceptor
axios.interceptors.response.use(function (response) {
    // Any status code that lie within the range of 2xx cause this function to trigger
    // Do something with response data
    return response;
  }, function (error) {
    // Any status codes that falls outside the range of 2xx cause this function to trigger
    // Do something with response error
    return Promise.reject(error);
  });
If you need to remove an interceptor later you can.

const myInterceptor = axios.interceptors.request.use(function () {/*...*/});
axios.interceptors.request.eject(myInterceptor);
You can also clear all interceptors for requests or responses.

const instance = axios.create();
instance.interceptors.request.use(function () {/*...*/});
instance.interceptors.request.clear(); // Removes interceptors from requests
instance.interceptors.response.use(function () {/*...*/});
instance.interceptors.response.clear(); // Removes interceptors from responses
You can add interceptors to a custom instance of axios.

const instance = axios.create();
instance.interceptors.request.use(function () {/*...*/});
When you add request interceptors, they are presumed to be asynchronous by default. This can cause a delay in the execution of your axios request when the main thread is blocked (a promise is created under the hood for the interceptor and your request gets put on the bottom of the call stack). If your request interceptors are synchronous you can add a flag to the options object that will tell axios to run the code synchronously and avoid any delays in request execution.

axios.interceptors.request.use(function (config) {
  config.headers.test = 'I am only a header!';
  return config;
}, null, { synchronous: true });
If you want to execute a particular interceptor based on a runtime check, you can add a runWhen function to the options object. The interceptor will not be executed if and only if the return of runWhen is false. The function will be called with the config object (don't forget that you can bind your own arguments to it as well.) This can be handy when you have an asynchronous request interceptor that only needs to run at certain times.

function onGetCall(config) {
  return config.method === 'get';
}
axios.interceptors.request.use(function (config) {
  config.headers.test = 'special get headers';
  return config;
}, null, { runWhen: onGetCall });
Multiple Interceptors
Given you add multiple response interceptors and when the response was fulfilled

then each interceptor is executed
then they are executed in the order they were added
then only the last interceptor's result is returned
then every interceptor receives the result of its predecessor
and when the fulfillment-interceptor throws
then the following fulfillment-interceptor is not called
then the following rejection-interceptor is called
once caught, another following fulfill-interceptor is called again (just like in a promise chain).
Read the interceptor tests for seeing all this in code.

Handling Errors
axios.get('/user/12345')
  .catch(function (error) {
    if (error.response) {
      // The request was made and the server responded with a status code
      // that falls out of the range of 2xx
      console.log(error.response.data);
      console.log(error.response.status);
      console.log(error.response.headers);
    } else if (error.request) {
      // The request was made but no response was received
      // `error.request` is an instance of XMLHttpRequest in the browser and an instance of
      // http.ClientRequest in node.js
      console.log(error.request);
    } else {
      // Something happened in setting up the request that triggered an Error
      console.log('Error', error.message);
    }
    console.log(error.config);
  });
Using the validateStatus config option, you can define HTTP code(s) that should throw an error.

axios.get('/user/12345', {
  validateStatus: function (status) {
    return status < 500; // Resolve only if the status code is less than 500
  }
})
Using toJSON you get an object with more information about the HTTP error.

axios.get('/user/12345')
  .catch(function (error) {
    console.log(error.toJSON());
  });
Cancellation
AbortController
Starting from v0.22.0 Axios supports AbortController to cancel requests in fetch API way:

const controller = new AbortController();

axios.get('/foo/bar', {
   signal: controller.signal
}).then(function(response) {
   //...
});
// cancel the request
controller.abort()
CancelToken üëédeprecated
You can also cancel a request using a CancelToken.

The axios cancel token API is based on the withdrawn cancelable promises proposal.

This API is deprecated since v0.22.0 and shouldn't be used in new projects

You can create a cancel token using the CancelToken.source factory as shown below:

const CancelToken = axios.CancelToken;
const source = CancelToken.source();

axios.get('/user/12345', {
  cancelToken: source.token
}).catch(function (thrown) {
  if (axios.isCancel(thrown)) {
    console.log('Request canceled', thrown.message);
  } else {
    // handle error
  }
});

axios.post('/user/12345', {
  name: 'new name'
}, {
  cancelToken: source.token
})

// cancel the request (the message parameter is optional)
source.cancel('Operation canceled by the user.');
You can also create a cancel token by passing an executor function to the CancelToken constructor:

const CancelToken = axios.CancelToken;
let cancel;

axios.get('/user/12345', {
  cancelToken: new CancelToken(function executor(c) {
    // An executor function receives a cancel function as a parameter
    cancel = c;
  })
});

// cancel the request
cancel();
Note: you can cancel several requests with the same cancel token/abort controller. If a cancellation token is already cancelled at the moment of starting an Axios request, then the request is cancelled immediately, without any attempts to make a real request.

During the transition period, you can use both cancellation APIs, even for the same request:

Using application/x-www-form-urlencoded format
URLSearchParams
By default, axios serializes JavaScript objects to JSON. To send data in the application/x-www-form-urlencoded format instead, you can use the URLSearchParams API, which is supported in the vast majority of browsers, and Node starting with v10 (released in 2018).

const params = new URLSearchParams({ foo: 'bar' });
params.append('extraparam', 'value');
axios.post('/foo', params);
Query string (Older browsers)
For compatibility with very old browsers, there is a polyfill available (make sure to polyfill the global environment).

Alternatively, you can encode data using the qs library:

const qs = require('qs');
axios.post('/foo', qs.stringify({ 'bar': 123 }));
Or in another way (ES6),

import qs from 'qs';
const data = { 'bar': 123 };
const options = {
  method: 'POST',
  headers: { 'content-type': 'application/x-www-form-urlencoded' },
  data: qs.stringify(data),
  url,
};
axios(options);
Older Node.js versions
For older Node.js engines, you can use the querystring module as follows:

const querystring = require('querystring');
axios.post('https://something.com/', querystring.stringify({ foo: 'bar' }));
You can also use the qs library.

NOTE: The qs library is preferable if you need to stringify nested objects, as the querystring method has known issues with that use case.

üÜï Automatic serialization to URLSearchParams
Axios will automatically serialize the data object to urlencoded format if the content-type header is set to "application/x-www-form-urlencoded".

const data = {
  x: 1,
  arr: [1, 2, 3],
  arr2: [1, [2], 3],
  users: [{name: 'Peter', surname: 'Griffin'}, {name: 'Thomas', surname: 'Anderson'}],
};

await axios.postForm('https://postman-echo.com/post', data,
  {headers: {'content-type': 'application/x-www-form-urlencoded'}}
);
The server will handle it as

  {
    x: '1',
    'arr[]': [ '1', '2', '3' ],
    'arr2[0]': '1',
    'arr2[1][0]': '2',
    'arr2[2]': '3',
    'arr3[]': [ '1', '2', '3' ],
    'users[0][name]': 'Peter',
    'users[0][surname]': 'griffin',
    'users[1][name]': 'Thomas',
    'users[1][surname]': 'Anderson'
  }
If your backend body-parser (like body-parser of express.js) supports nested objects decoding, you will get the same object on the server-side automatically

  var app = express();
  
  app.use(bodyParser.urlencoded({ extended: true })); // support encoded bodies
  
  app.post('/', function (req, res, next) {
     // echo body as JSON
     res.send(JSON.stringify(req.body));
  });

  server = app.listen(3000);
Using multipart/form-data format
FormData
To send the data as a multipart/formdata you need to pass a formData instance as a payload. Setting the Content-Type header is not required as Axios guesses it based on the payload type.

const formData = new FormData();
formData.append('foo', 'bar');

axios.post('https://httpbin.org/post', formData);
In node.js, you can use the form-data library as follows:

const FormData = require('form-data');
 
const form = new FormData();
form.append('my_field', 'my value');
form.append('my_buffer', new Buffer(10));
form.append('my_file', fs.createReadStream('/foo/bar.jpg'));

axios.post('https://example.com', form)
üÜï Automatic serialization to FormData
Starting from v0.27.0, Axios supports automatic object serialization to a FormData object if the request Content-Type header is set to multipart/form-data.

The following request will submit the data in a FormData format (Browser & Node.js):

import axios from 'axios';

axios.post('https://httpbin.org/post', {x: 1}, {
  headers: {
    'Content-Type': 'multipart/form-data'
  }
}).then(({data})=> console.log(data));
In the node.js build, the (form-data) polyfill is used by default.

You can overload the FormData class by setting the env.FormData config variable, but you probably won't need it in most cases:

const axios= require('axios');
var FormData = require('form-data');

axios.post('https://httpbin.org/post', {x: 1, buf: new Buffer(10)}, {
  headers: {
    'Content-Type': 'multipart/form-data'
  }
}).then(({data})=> console.log(data));
Axios FormData serializer supports some special endings to perform the following operations:

{} - serialize the value with JSON.stringify
[] - unwrap the array-like object as separate fields with the same key
NOTE: unwrap/expand operation will be used by default on arrays and FileList objects

FormData serializer supports additional options via config.formSerializer: object property to handle rare cases:

visitor: Function - user-defined visitor function that will be called recursively to serialize the data object to a FormData object by following custom rules.

dots: boolean = false - use dot notation instead of brackets to serialize arrays and objects;

metaTokens: boolean = true - add the special ending (e.g user{}: '{"name": "John"}') in the FormData key. The back-end body-parser could potentially use this meta-information to automatically parse the value as JSON.

indexes: null|false|true = false - controls how indexes will be added to unwrapped keys of flat array-like objects

null - don't add brackets (arr: 1, arr: 2, arr: 3)
false(default) - add empty brackets (arr[]: 1, arr[]: 2, arr[]: 3)
true - add brackets with indexes (arr[0]: 1, arr[1]: 2, arr[2]: 3)
Let's say we have an object like this one:

const obj = {
  x: 1,
  arr: [1, 2, 3],
  arr2: [1, [2], 3],
  users: [{name: 'Peter', surname: 'Griffin'}, {name: 'Thomas', surname: 'Anderson'}],
  'obj2{}': [{x:1}]
};
The following steps will be executed by the Axios serializer internally:

const formData= new FormData();
formData.append('x', '1');
formData.append('arr[]', '1');
formData.append('arr[]', '2');
formData.append('arr[]', '3');
formData.append('arr2[0]', '1');
formData.append('arr2[1][0]', '2');
formData.append('arr2[2]', '3');
formData.append('users[0][name]', 'Peter');
formData.append('users[0][surname]', 'Griffin');
formData.append('users[1][name]', 'Thomas');
formData.append('users[1][surname]', 'Anderson');
formData.append('obj2{}', '[{"x":1}]');
Axios supports the following shortcut methods: postForm, putForm, patchForm which are just the corresponding http methods with the Content-Type header preset to multipart/form-data.

Files Posting
You can easily sumbit a single file

await axios.postForm('https://httpbin.org/post', {
  'myVar' : 'foo',
  'file': document.querySelector('#fileInput').files[0] 
});
or multiple files as multipart/form-data.

await axios.postForm('https://httpbin.org/post', {
  'files[]': document.querySelector('#fileInput').files 
});
FileList object can be passed directly:

await axios.postForm('https://httpbin.org/post', document.querySelector('#fileInput').files)
All files will be sent with the same field names: files[].

üÜï HTML Form Posting (browser)
Pass HTML Form element as a payload to submit it as multipart/form-data content.

await axios.postForm('https://httpbin.org/post', document.querySelector('#htmlForm'));
FormData and HTMLForm objects can also be posted as JSON by explicitly setting the Content-Type header to application/json:

await axios.post('https://httpbin.org/post', document.querySelector('#htmlForm'), {
  headers: {
    'Content-Type': 'application/json'
  }
})
For example, the Form

<form id="form">
  <input type="text" name="foo" value="1">
  <input type="text" name="deep.prop" value="2">
  <input type="text" name="deep prop spaced" value="3">
  <input type="text" name="baz" value="4">
  <input type="text" name="baz" value="5">

  <select name="user.age">
    <option value="value1">Value 1</option>
    <option value="value2" selected>Value 2</option>
    <option value="value3">Value 3</option>
  </select>

  <input type="submit" value="Save">
</form>
will be submitted as the following JSON object:

{
  "foo": "1",
  "deep": {
    "prop": {
      "spaced": "3"
    }
  },
  "baz": [
    "4",
    "5"
  ],
  "user": {
    "age": "value2"
  }
}
Sending Blobs/Files as JSON (base64) is not currently supported.

Semver
Until axios reaches a 1.0 release, breaking changes will be released with a new minor version. For example 0.5.1, and 0.5.4 will have the same API, but 0.6.0 will have breaking changes.

Promises
axios depends on a native ES6 Promise implementation to be supported. If your environment doesn't support ES6 Promises, you can polyfill.

TypeScript
axios includes TypeScript definitions and a type guard for axios errors.

let user: User = null;
try {
  const { data } = await axios.get('/user?ID=12345');
  user = data.userDetails;
} catch (error) {
  if (axios.isAxiosError(error)) {
    handleAxiosError(error);
  } else {
    handleUnexpectedError(error);
  }
}
Online one-click setup
You can use Gitpod, an online IDE(which is free for Open Source) for contributing or running the examples online.

Open in Gitpod

Resources
Changelog
Upgrade Guide
Ecosystem
Contributing Guide
Code of Conduct
Credits
axios is heavily inspired by the $http service provided in AngularJS. Ultimately axios is an effort to provide a standalone $http-like service for use outside of AngularJS.

License
MIT


Superagent.

superagent
build status code coverage code style styled with prettier made with lass license

Small progressive client-side HTTP request library, and Node.js module with the same API, supporting many high-level HTTP client features. Maintained for Forward Email and Lad.

Table of Contents
Install
Usage
Node
Browser
Supported Platforms
Required Browser Features
Plugins
Upgrading from previous versions
Contributors
License
Install
npm:

npm install superagent
yarn:

yarn add superagent
Usage
Node
const superagent = require('superagent');

// callback
superagent
  .post('/api/pet')
  .send({ name: 'Manny', species: 'cat' }) // sends a JSON post body
  .set('X-API-Key', 'foobar')
  .set('accept', 'json')
  .end((err, res) => {
    // Calling the end function will send the request
  });

// promise with then/catch
superagent.post('/api/pet').then(console.log).catch(console.error);

// promise with async/await
(async () => {
  try {
    const res = await superagent.post('/api/pet');
    console.log(res);
  } catch (err) {
    console.error(err);
  }
})();
Browser
The browser-ready, minified version of superagent is only 50 KB (minified and gzipped).

Browser-ready versions of this module are available via jsdelivr, unpkg, and also in the node_modules/superagent/dist folder in downloads of the superagent package.

Note that we also provide unminified versions with .js instead of .min.js file extensions.

VanillaJS
This is the solution for you if you're just using <script> tags everywhere!

<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=WeakRef,BigInt"></script>
<script src="https://cdn.jsdelivr.net/npm/superagent"></script>
<!-- if you wish to use unpkg.com instead: -->
<!-- <script src="https://unpkg.com/superagent"></script> -->
<script type="text/javascript">
  (function() {
    // superagent is exposed as `window.superagent`
    // if you wish to use "request" instead please
    // uncomment the following line of code:
    // `window.request = superagent;`
    superagent
      .post('/api/pet')
      .send({ name: 'Manny', species: 'cat' }) // sends a JSON post body
      .set('X-API-Key', 'foobar')
      .set('accept', 'json')
      .end(function (err, res) {
        // Calling the end function will send the request
      });
  })();
</script>
Bundler
If you are using browserify, webpack, rollup, or another bundler, then you can follow the same usage as Node above.

Supported Platforms
Node: v14.18.0+

Browsers (see .browserslistrc):

npx browserslist
and_chr 102
and_ff 101
and_qq 10.4
and_uc 12.12
android 101
chrome 103
chrome 102
chrome 101
chrome 100
edge 103
edge 102
edge 101
firefox 101
firefox 100
firefox 91
ios_saf 15.5
ios_saf 15.4
ios_saf 15.2-15.3
ios_saf 15.0-15.1
ios_saf 14.5-14.8
ios_saf 14.0-14.4
ios_saf 12.2-12.5
kaios 2.5
op_mini all
op_mob 64
opera 86
opera 85
safari 15.5
safari 15.4
samsung 17.0
samsung 16.0
Required Browser Features
We recommend using https://cdnjs.cloudflare.com/polyfill/ (specifically with the bundle mentioned in VanillaJS above):

<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=WeakRef,BigInt"></script>
WeakRef is not supported in Opera 85, iOS Safari 12.2-12.5
BigInt is not supported in iOS Safari 12.2-12.5
Plugins
SuperAgent is easily extended via plugins.

const nocache = require('superagent-no-cache');
const superagent = require('superagent');
const prefix = require('superagent-prefix')('/static');

superagent
  .get('/some-url')
  .query({ action: 'edit', city: 'London' }) // query string
  .use(prefix) // Prefixes *only* this request
  .use(nocache) // Prevents caching of *only* this request
  .end((err, res) => {
    // Do something
  });
Existing plugins:

superagent-no-cache - prevents caching by including Cache-Control header
superagent-prefix - prefixes absolute URLs (useful in test environment)
superagent-suffix - suffix URLs with a given path
superagent-mock - simulate HTTP calls by returning data fixtures based on the requested URL
superagent-mocker ‚Äî simulate REST API
superagent-cache - A global SuperAgent patch with built-in, flexible caching
superagent-cache-plugin - A SuperAgent plugin with built-in, flexible caching
superagent-jsonapify - A lightweight json-api client addon for superagent
superagent-serializer - Converts server payload into different cases
superagent-httpbackend - stub out requests using AngularJS' $httpBackend syntax
superagent-throttle - queues and intelligently throttles requests
superagent-charset - add charset support for node's SuperAgent
superagent-verbose-errors - include response body in error messages for failed requests
superagent-declare - A simple declarative API for SuperAgent
superagent-node-http-timings - measure http timings in node.js
superagent-cheerio - add cheerio to your response content automatically. Adds res.$ for HTML and XML response bodies.
@certible/superagent-aws-sign - Sign AWS endpoint requests, it uses the aws4 to authenticate the SuperAgent requests
Please prefix your plugin with superagent-* so that it can easily be found by others.

For SuperAgent extensions such as couchdb and oauth visit the wiki.

Upgrading from previous versions
Please see GitHub releases page for the current changelog.

Our breaking changes are mostly in rarely used functionality and from stricter error handling.

6.0 to 6.1
Browser behaviour changed to match Node when serializing application/x-www-form-urlencoded, using arrayFormat: 'indices' semantics of qs library. (See: https://www.npmjs.com/package/qs#stringifying)
5.x to 6.x:
Retry behavior is still opt-in, however we now have a more fine-grained list of status codes and error codes that we retry against (see updated docs)
A specific issue with Content-Type matching not being case-insensitive is fixed
Set is now required for IE 9, see Required Browser Features for more insight
4.x to 5.x:
We've implemented the build setup of Lass to simplify our stack and linting
Unminified browserified build size has been reduced from 48KB to 20KB (via tinyify and the latest version of Babel using @babel/preset-env and .browserslistrc)
Linting support has been added using caniuse-lite and eslint-plugin-compat
We can now target what versions of Node we wish to support more easily using .babelrc
3.x to 4.x:
Ensure you're running Node 6 or later. We've dropped support for Node 4.
We've started using ES6 and for compatibility with Internet Explorer you may need to use Babel.
We suggest migrating from .end() callbacks to .then() or await.
2.x to 3.x:
Ensure you're running Node 4 or later. We've dropped support for Node 0.x.
Test code that calls .send() multiple times. Invalid calls to .send() will now throw instead of sending garbage.
1.x to 2.x:
If you use .parse() in the browser version, rename it to .serialize().
If you rely on undefined in query-string values being sent literally as the text "undefined", switch to checking for missing value instead. ?key=undefined is now ?key (without a value).
If you use .then() in Internet Explorer, ensure that you have a polyfill that adds a global Promise object.
0.x to 1.x:
Instead of 1-argument callback .end(function(res){}) use .then(res => {}).

License
MIT ¬© TJ Holowaychuk


Cacheable-request.

Documentation
npm
Search packages.
cacheable-request
TypeScript icon, indicating that this package has built-in type declarations
13.0.7 ‚Ä¢ Public ‚Ä¢ 

cacheable-request
Wrap native HTTP requests with RFC compliant cache support

codecov tests npm npm license

RFC 7234 compliant HTTP caching for native Node.js HTTP/HTTPS requests. Caching works out of the box in memory or is easily pluggable with a wide range of storage adapters.

Note: This is a low level wrapper around the core HTTP modules, it's not a high level request library.

Table of Contents
Latest Changes
Features
Install and Usage
Storage Adapters
API
Using Hooks
Contributing
Ask a Question
License (MIT)
Latest Changes
Breaking Changes with v13.0.0
Keyv has been updated to version 5. With this update, you can no longer pass in a connection string directly to the CacheableRequest constructor. Instead, you should pass in a Keyv or Keyv storage adapter instance.

Breaking Changes with v10.0.0
This release contains breaking changes. This is the new way to use this package.

Usage Before v10
import http from 'http';
import CacheableRequest from 'cacheable-request';

// Then instead of
const req = http.request('http://example.com', cb);
req.end();

// You can do
const cacheableRequest = new CacheableRequest(http.request);
const cacheReq = cacheableRequest('http://example.com', cb);
cacheReq.on('request', req => req.end());
// Future requests to 'example.com' will be returned from cache if still valid

// You pass in any other http.request API compatible method to be wrapped with cache support:
const cacheableRequest = new CacheableRequest(https.request);
const cacheableRequest = new CacheableRequest(electron.net);
Usage After v10.1.0
import CacheableRequest from 'cacheable-request';

// Now You can do
const cacheableRequest = new CacheableRequest(http.request).request();
const cacheReq = cacheableRequest('http://example.com', cb);
cacheReq.on('request', req => req.end());
// Future requests to 'example.com' will be returned from cache if still valid

// You pass in any other http.request API compatible method to be wrapped with cache support:
const cacheableRequest = new CacheableRequest(https.request).request();
const cacheableRequest = new CacheableRequest(electron.net).request();
The biggest change is that when you do a new CacheableRequest you now want to call request method will give you the instance to use.

- const cacheableRequest = new CacheableRequest(http.request);
+ const cacheableRequest = new CacheableRequest(http.request).request();
ESM Support in version 9 and higher.
We are now using pure esm support in our package. If you need to use commonjs you can use v8 or lower. To learn more about using ESM please read this from sindresorhus: https://gist.github.com/sindresorhus/a39789f98801d908bbc7ff3ecc99d99c

Features
Only stores cacheable responses as defined by RFC 7234
Fresh cache entries are served directly from cache
Stale cache entries are revalidated with If-None-Match/If-Modified-Since headers
304 responses from revalidation requests use cached body
Updates Age header on cached responses
Can completely bypass cache on a per request basis
In memory cache by default
Official support for Redis, Memcache, Etcd, MongoDB, SQLite, PostgreSQL and MySQL storage adapters
Easily plug in your own or third-party storage adapters
If DB connection fails, cache is automatically bypassed (disabled by default)
Adds cache support to any existing HTTP code with minimal changes
Uses http-cache-semantics internally for HTTP RFC 7234 compliance
Install and Usage
npm install cacheable-request
import http from 'http';
import CacheableRequest from 'cacheable-request';

// Then instead of
const req = http.request('http://example.com', cb);
req.end();

// You can do
const cacheableRequest = new CacheableRequest(http.request).createCacheableRequest();
const cacheReq = cacheableRequest('http://example.com', cb);
cacheReq.on('request', req => req.end());
// Future requests to 'example.com' will be returned from cache if still valid

// You pass in any other http.request API compatible method to be wrapped with cache support:
const cacheableRequest = new CacheableRequest(https.request).createCacheableRequest();
const cacheableRequest = new CacheableRequest(electron.net).createCacheableRequest();
Storage Adapters
cacheable-request uses Keyv to support a wide range of storage adapters.

For example, to use Redis as a cache backend, you just need to install the official Redis Keyv storage adapter:

npm install @keyv/redis
And then you can pass CacheableRequest your connection string:

import KeyvRedis from '@keyv/redis';
import CacheableRequest from 'cacheable-request';

const keyvRedis = new KeyvRedis('redis://localhost:6379');
const cacheableRequest = new CacheableRequest(http.request, KeyvRedis).createCacheableRequest();
View all official Keyv storage adapters.

Keyv also supports anything that follows the Map API so it's easy to write your own storage adapter or use a third-party solution.

e.g The following are all valid storage adapters

const storageAdapter = new Map();
// or
const storageAdapter = require('./my-storage-adapter');
// or
const QuickLRU = require('quick-lru');
const storageAdapter = new QuickLRU({ maxSize: 1000 });

const cacheableRequest = new CacheableRequest(http.request, storageAdapter).createCacheableRequest();
View the Keyv docs for more information on how to use storage adapters.

API
new cacheableRequest(request, [storageAdapter])
Returns the provided request function wrapped with cache support.

request
Type: function

Request function to wrap with cache support. Should be http.request or a similar API compatible request function.

storageAdapter
Type: Keyv storage adapter
Default: new Map()

A Keyv storage adapter instance, or connection string if using with an official Keyv storage adapter.

Instance
cacheableRequest(opts, [cb])
Returns an event emitter.

opts
Type: object, string

Any of the default request functions options.
Any http-cache-semantics options.
Any of the following:
opts.cache
Type: boolean
Default: true

If the cache should be used. Setting this to false will completely bypass the cache for the current request.

opts.strictTtl
Type: boolean
Default: false

If set to true once a cached resource has expired it is deleted and will have to be re-requested.

If set to false (default), after a cached resource's TTL expires it is kept in the cache and will be revalidated on the next request with If-None-Match/If-Modified-Since headers.

opts.maxTtl
Type: number
Default: undefined

Limits TTL. The number represents milliseconds.

opts.automaticFailover
Type: boolean
Default: false

When set to true, if the DB connection fails we will automatically fallback to a network request. DB errors will still be emitted to notify you of the problem even though the request callback may succeed.

opts.forceRefresh
Type: boolean
Default: false

Forces refreshing the cache. If the response could be retrieved from the cache, it will perform a new request and override the cache instead.

cb
Type: function

The callback function which will receive the response as an argument.

The response can be either a Node.js HTTP response stream or a responselike object. The response will also have a fromCache property set with a boolean value.

.on('request', request)
request event to get the request object of the request.

Note: This event will only fire if an HTTP request is actually made, not when a response is retrieved from cache. However, you should always handle the request event to end the request and handle any potential request errors.

.on('response', response)
response event to get the response object from the HTTP request or cache.

.on('error', error)
error event emitted in case of an error with the cache.

Errors emitted here will be an instance of CacheableRequest.RequestError or CacheableRequest.CacheError. You will only ever receive a RequestError if the request function throws (normally caused by invalid user input). Normal request errors should be handled inside the request event.

To properly handle all error scenarios you should use the following pattern:

cacheableRequest('example.com', cb)
  .on('error', err => {
    if (err instanceof CacheableRequest.CacheError) {
      handleCacheError(err); // Cache error
    } else if (err instanceof CacheableRequest.RequestError) {
      handleRequestError(err); // Request function thrown
    }
  })
  .on('request', req => {
    req.on('error', handleRequestError); // Request error emitted
    req.end();
  });
Note: Database connection errors are emitted here, however cacheable-request will attempt to re-request the resource and bypass the cache on a connection error. Therefore a database connection error doesn't necessarily mean the request won't be fulfilled.

Using Hooks
Hooks have been implemented since version v9 and are very useful to modify response before saving it in cache. You can use hooks to modify response before saving it in cache.

Add Hooks
The hook will pre compute response right before saving it in cache. You can include many hooks and it will run in order you add hook on response object.

import http from 'http';
import CacheableRequest from 'cacheable-request';

const cacheableRequest = new CacheableRequest(request, cache).request();

// adding a hook to decompress response
cacheableRequest.addHook('onResponse', async (value: CacheValue, response: any) => {
  const buffer = await pm(gunzip)(value.body);
  value.body = buffer.toString();
  return value;
});
here is also an example of how to add in the remote address

import CacheableRequest, {CacheValue} from 'cacheable-request';

const cacheableRequest = new CacheableRequest(request, cache).request();
cacheableRequest.addHook('onResponse', (value: CacheValue, response: any) => {
  if (response.connection) {
    value.remoteAddress = response.connection.remoteAddress;
  }

  return value;
});
Remove Hooks
You can also remove hook by using below

CacheableRequest.removeHook('onResponse');
How to Contribute
Cacheable-Request is an open source package and community driven that is maintained regularly. In addition we have a couple of other guidelines for review:

CODE_OF_CONDUCT.md - Our code of conduct
CONTRIBUTING.md - How to contribute to this project
SECURITY.md - Security guidelines and supported versions
Post an Issue
To post an issue, navigate to the "Issues" tab in the main repository, and then select "New Issue." Enter a clear title describing the issue, as well as a description containing additional relevant information. Also select the label that best describes your issue type. For a bug report, for example, create an issue with the label "bug." In the description field, Be sure to include replication steps, as well as any relevant error messages.

If you're reporting a security violation, be sure to check out the project's security policy.

Please also refer to our Code of Conduct for more information on how to report issues.

License and Copyright.
MIT ¬© Luke Childs 2017-2021 and Jared Wray 2022+

Readme.
Keywords HTTPHTTPScachecachinglayercacheableRFC 7234RFC7234compliant
Package Sidebar
Install
npm i cacheable-request.
Repository
github.com/jaredwray/cacheable.
github.com/jaredwray/cacheable#readme.

Version 13.0.7 . License MIT.

GotqL

GotQL

Write GraphQL queries as objects instead of strings.

  JavaScript Style Guide
This is a better implementation of the GraphQL query API via NodeJS, created as a wrapper of Got. It works like a transpiler, with a built in HTTPRequest Client (Got), allowing you to write your GraphQL queries as Javascript Objects instead of strings.

Built because manipulating strings is a real pain.

Table of Contents
Table of Contents
Install
Basic Usage
What is it?
Motivation
API
Option Object
Returns
The JSON query format
Description
Examples
Simple query
Named query
Query with simple args
Query with variables
Nested fields
Enum args
Fragment Support
Contributing to this project
Install
$ npm install gotql
Or

$ yarn install gotql
Basic Usage
const gotQl = require('gotql')

const query = {
  operation: {
    name: 'users',
    fields: ['name', 'age', 'id']
  }
}

const options = {
  headers: {
    Authorization: 'Bearer <token>'
  },
  debug: false,
  useHttp2: false
}

gotQL
  .query('mygraphqlendpoint.com.br/api', query, options)
  .then((response) => console.log(response.data))
  .catch(console.error)
What is it?
GotQL is a better interface for GraphQL queries. It provides a way for developers to run queries using JSON instead of strings. Which is a way more usable data format than the string itself.

See more on: https://hasura.io/blog/fluent-graphql-clients-how-to-write-queries-like-a-boss/

Motivation
Manipulating strings is very smelly, even on dynamically typed languages. So, in order to avoid things such as this:

Which can be translated to something waay more readable in a JSON format like this:

const mutation = {
  operation: {
    name: 'addLog',
    args: {
      logType: literal`status_change`, // Enum Value
      fromState: variables.fromState,
      toState: variables.toState,
      idUser: variables.idUser,
      idCampaign: variables.idCampaign,
      owner: {
        ownerType: variables.ownerType,
        username: variables.username,
        picture: variables.picture,
        name: variables.name,
        id: variables.id
      }
    },
    fields: ['uuid']
  }
}
This is why GotQL was created.

API
gotQl.query(graphQLEndpoint, query, [options])
Description: Performs a graphQL query
GraphQLEndpoint

Type: string
Description: The GraphQL endpoint to query on
query

Type: object
Description: The JSON-typed query following the json-query format
options

See option object for more information.

gotQl.mutation(graphQLEndpoint, query, [options])
Description: Performs a graphQL mutation
GraphQLEndpoint

Type: string
Description: The GraphQL endpoint to query on
query

Type: object
Description: The JSON-typed query following the json-query format
options

See option object for more information.

gotQl.parser(query, type)
Description: Parses a JSON-Like query and returns the query's string
query

Type: object
Description: The JSON-typed query following the json-query format
type

Type: string
Description: Must be either 'query' or 'mutation'
Option Object
Both gotql.query and gotql.mutation accept an optional user option object with the following API:

Type: object
Description: The option object with the following properties.
errorStatusCode: Default HTTP status code to be returned on error
Type: number
headers: Additional headers to be sent
Type: object, in the form of [headerName: string]: headerValue: string
gotInstance: Customized Got instance to be used when calling the endpoint
Type: got. Internally this will be called as got.post(prependHttp(endPoint), gotPayload)
useHttp2: Boolean defining if the call should be made using HTTP2, defaults to false (see release 11 of got)
Type: boolean
Note: GotQL uses debug internally as default debugger, so you can set debug levels by setting the DEBUG environment variable. These are the current levels:

gotql:info
gotql:info:parser
gotql:info:runner
gotql:errors
Returns
All methods return a string like this:

const response = 'query { test { name args } }'
The JSON query format
The JSON format gotQL uses is a simple and intuitive description based on the anatomy of a GraphQL query blog post.

This is a generic model of a JSONLike query:

const query = {
  name?: string,
  operation: {
    name: string,
    alias?: string,
    args?: { [argName: string]: any } | {
      [argName: string]: {
        value: string,
        escape: boolean
      }
    },
    fields: (string | {
      [fieldName: string]: [{
        args?: { [argName: string]: any } | {
          [argName: string]: {
            value: string,
            escape: boolean
          }
        },
        fields?: (string | { [fieldName: string]: [any] })[]
      }]
    })[]
  },
  variables?: {
    [varName: string]: {
      type: string,
      value: string
    }
  }
}
Description
Query:
Type: object
Description: The full query object
Properties:
name: [optional]: Query name
Type: string
variables: [optional] Query variable declaration
Type: object with signature like [varName: string]: { type: string, value: string }
Properties:
varName: Variable name
Type: string
type: Variable type. Can be a GraphQL definition of type (i.e: string!)
Type: string
value: Variable value
Type: any
operation: The query operation (action that will be executed)
Type: object
Properties:
name: The operation name
Type: string
alias: [optional] An alias to give the operation
Type: string
args: [optional] The operation args
Type: [argName: string]: any or a detailed arg object
Simple args: An object where the key is the argument name and its value. Accepts variables in the format of argName: '$value'
Example: args { name: 'myName' }
Detailed args: A tagged template. This will give more control over escaping (mostly to use enums). Argument name should be the key
Type: tagged template
Examples: args: { status: literal`an_enum` } should output operation (status: an_enum)...
fields: The field list to get back from the operation
Type: An array of object (to use nested fields) or string, or both.
Properties (for nested fields):
Type: object where the field name is the key
fields: Recursive definition, accepts another array just like the fields above.
args: [optional] The field args
Type: [argName: string]: any or a detailed arg object
Simple args: An object where the key is the argument name and its value. Accepts variables in the format of argName: '$value'
Example: args { name: 'myName' }
Detailed args: A tagged template. This will give more control over escaping (mostly to use enums). Argument name should be the key
Type: tagged template
Examples: args: { status: literal`an_enum` } should output operation (status: an_enum)...
fragments: The fragments of the query, see Fragments Support for more information
Type: string[]
Examples
Simple query
const query = {
  operation: {
    name: 'users',
    fields: ['name', 'age']
  }
}
Outputs:

query { users { name age } }
Named query
const query = {
  name: 'myQuery',
  operation: {
    name: 'users',
    fields: ['name', 'age']
  }
}
Outputs:

query myQuery { users { name age } }
Query with simple args
const query = {
  operation: {
    name: 'users',
    args: {
      name: 'Joe'
    },
    fields: ['name', 'age']
  }
}
Outputs:

query { users(name: "Joe") { name age } }
Query with variables
const query = {
  variables: {
    name: {
      type: 'string!',
      value: 'Joe'
    }
  },
  operation: {
    name: 'users',
    args: {
      name: '$name'
    },
    fields: ['name', 'age']
  }
}
Outputs:

query ($name: string!) { users(name: $name) { name age } }
Variables are sent on a separate object to graphQL.

{
  "variables": { "name": "Joe" }
}
Nested fields
const query = {
  operation: {
    name: 'users',
    fields: [
      'name',
      'age',
      {
        friends: {
          fields: ['name', 'age']
        }
      }
    ]
  }
}
Outputs:

query { users { name age friends { name age } } }
Recursive fields can go forever.

Enum and literal args
Enum or literal values should not be escaped, to do that, GotQL has a helper called literal which can be used to tell the query that value will not be escaped:

const { literal } = require('gotql')

const query = {
  operation: {
    name: 'users',
    args: {
      type: literal`internal`
    },
    fields: ['name', 'age']
  }
}
The code above outputs:

query { users(type: internal) { name age } }
The literal helper is just a shorthand to the old-style {value: string, escape: boolean} object like below:

const query = {
  operation: {
    name: 'users',
    args: {
      type: {
        value: 'internal',
        escape: false
      }
    },
    fields: ['name', 'age']
  }
}
If literal is omitted, or if escape is set to true, the output would be:

query { users(type: "internal") { name age } }
Note: Variables such as described here will not be recognized. If the arg object is not an [argName]: value, variables will not pass through the definition check (GotQL warns if a variable is not declared but used on operation).

Fragment support
Fragment support is in an alpha state (see #55), this means that, while the lib supports fragments, it's not as pretty or as tested as I'd like it to be, but PR's are welcome if you want to use it thoroughly.

You can use fragments by adding a new key in the query JSON, besides operation, just like you do with variables. This key is called fragments and it's an array of strings that represent fragments in operations, for example:

const query = {
  operation: {
    fields: ['f1']
  },
  fragments: [`fragment Test on Character { name id }`]
}
You can then reference those fragments using the literal struct fragment:

const query = {
  operation: {
    fields: [fragment`Test`]
  },
  fragments: [`fragment Test on Character { name id }`]
}
Contributing to this project
Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.

Hey! If you want to contribute, please read the contributing guidelines emoji people:smile

Contributors
Code Contributors
This project exists thanks to all the people who contribute. [Contribute]. 

Financial Contributors
Become a financial contributor and help us sustain our community. [Contribute]

Individuals

Organizations
Support this project with your organization. Your logo will show up here with a link to your website. [Contribute]

         
Global-Agent.

global-agent
Travis build status Coveralls NPM version Canonical Code.

Global HTTP/HTTPS proxy configurable using environment variables.

Usage
Setup proxy using global-agent/bootstrap
Setup proxy using bootstrap routine
Runtime configuration
Exclude URLs
Enable logging
API
createGlobalProxyAgent
Environment variables
global.GLOBAL_AGENT
Supported libraries
FAQ
What is the reason global-agent overrides explicitly configured HTTP(S) agent?
What is the reason global-agent/bootstrap does not use HTTP_PROXY?
What is the difference from global-tunnel and tunnel?
Usage
Setup proxy using global-agent/bootstrap
To configure HTTP proxy:

Import global-agent/bootstrap.
Export HTTP proxy address as GLOBAL_AGENT_HTTP_PROXY environment variable.
Code:

import 'global-agent/bootstrap';

// or:
// import {bootstrap} from 'global-agent';
// bootstrap();

Bash:

$ export GLOBAL_AGENT_HTTP_PROXY=http://127.0.0.1:8080

Alternatively, you can preload module using Node.js --require, -r configuration, e.g.

$ export GLOBAL_AGENT_HTTP_PROXY=http://127.0.0.1:8080
$ node -r 'global-agent/bootstrap' your-script.js

Setup proxy using bootstrap routine
Instead of importing a self-initialising script with side-effects as demonstrated in the setup proxy using global-agent/bootstrap documentation, you can import bootstrap routine and explicitly evaluate the bootstrap logic, e.g.

import {
  bootstrap
} from 'global-agent';

bootstrap();

This is useful if you need to conditionally bootstrap global-agent, e.g.

import {
  bootstrap
} from 'global-agent';
import globalTunnel from 'global-tunnel-ng';

const MAJOR_NODEJS_VERSION = parseInt(process.version.slice(1).split('.')[0], 10);

if (MAJOR_NODEJS_VERSION >= 10) {
  // `global-agent` works with Node.js v10 and above.
  bootstrap();
} else {
  // `global-tunnel-ng` works only with Node.js v10 and below.
  globalTunnel.initialize();
}

Setup proxy using createGlobalProxyAgent
If you do not want to use global.GLOBAL_AGENT variable, then you can use createGlobalProxyAgent to instantiate a controlled instance of global-agent, e.g.

import {
  createGlobalProxyAgent
} from 'global-agent';

const globalProxyAgent = createGlobalProxyAgent();

Unlike bootstrap routine, createGlobalProxyAgent factory does not create global.GLOBAL_AGENT variable and does not guard against multiple initializations of global-agent. The result object of createGlobalProxyAgent is equivalent to global.GLOBAL_AGENT.

Runtime configuration
global-agent/bootstrap script copies process.env.GLOBAL_AGENT_HTTP_PROXY value to global.GLOBAL_AGENT.HTTP_PROXY and continues to use the latter variable.

You can override the global.GLOBAL_AGENT.HTTP_PROXY value at runtime to change proxy behaviour, e.g.

http.get('http://127.0.0.1:8000');

global.GLOBAL_AGENT.HTTP_PROXY = 'http://127.0.0.1:8001';

http.get('http://127.0.0.1:8000');

global.GLOBAL_AGENT.HTTP_PROXY = 'http://127.0.0.1:8002';

The first HTTP request is going to use http://127.0.0.1:8001 proxy and the second request is going to use http://127.0.0.1:8002.

All global-agent configuration is available under global.GLOBAL_AGENT namespace.

Exclude URLs
The GLOBAL_AGENT_NO_PROXY environment variable specifies a pattern of URLs that should be excluded from proxying. GLOBAL_AGENT_NO_PROXY value is a comma-separated list of domain names. Asterisks can be used as wildcards, e.g.

export GLOBAL_AGENT_NO_PROXY='*.foo.com,baz.com'

says to contact all machines with the 'foo.com' TLD and 'baz.com' domains directly.

Separate proxy for HTTPS
The environment variable GLOBAL_AGENT_HTTPS_PROXY can be set to specify a separate proxy for HTTPS requests. When this variable is not set GLOBAL_AGENT_HTTP_PROXY is used for both HTTP and HTTPS requests.

Enable logging
global-agent is using roarr logger to log HTTP requests and response (HTTP status code and headers), e.g.

{"context":{"program":"global-agent","namespace":"Agent","logLevel":10,"destination":"http://gajus.com","proxy":"http://127.0.0.1:8076"},"message":"proxying request","sequence":1,"time":1556269669663,"version":"1.0.0"}
{"context":{"program":"global-agent","namespace":"Agent","logLevel":10,"headers":{"content-type":"text/plain","content-length":"2","date":"Fri, 26 Apr 2019 12:07:50 GMT","connection":"close"},"requestId":6,"statusCode":200},"message":"proxying response","sequence":2,"time":1557133856955,"version":"1.0.0"}

Export ROARR_LOG=true environment variable to enable log printing to stdout.

Use roarr-cli program to pretty-print the logs.

API
createGlobalProxyAgent
/**
 * @property environmentVariableNamespace Defines namespace of `HTTP_PROXY`, `HTTPS_PROXY` and `NO_PROXY` environment variables. (Default: `GLOBAL_AGENT_`)
 * @property forceGlobalAgent Forces to use `global-agent` HTTP(S) agent even when request was explicitly constructed with another agent. (Default: `true`)
 * @property socketConnectionTimeout Destroys socket if connection is not established within the timeout. (Default: `60000`)
 */
type ProxyAgentConfigurationInputType = {|
  +environmentVariableNamespace?: string,
  +forceGlobalAgent?: boolean,
  +socketConnectionTimeout?: number,
|};

(configurationInput: ProxyAgentConfigurationInputType) => ProxyAgentConfigurationType;

Environment variables
Name Description Default
GLOBAL_AGENT_ENVIRONMENT_VARIABLE_NAMESPACE Defines namespace of HTTP_PROXY, HTTPS_PROXY and NO_PROXY environment variables. GLOBAL_AGENT_
GLOBAL_AGENT_FORCE_GLOBAL_AGENT Forces to use global-agent HTTP(S) agent even when request was explicitly constructed with another agent. true
GLOBAL_AGENT_SOCKET_CONNECTION_TIMEOUT Destroys socket if connection is not established within the timeout. 60000
${NAMESPACE}HTTP_PROXY Sets the initial proxy controller HTTP_PROXY value. N/A
${NAMESPACE}HTTPS_PROXY Sets the initial proxy controller HTTPS_PROXY value. N/A
${NAMESPACE}NO_PROXY Sets the initial proxy controller NO_PROXY value. N/A
global.GLOBAL_AGENT
global.GLOBAL_AGENT is initialized by bootstrap routine.

global.GLOBAL_AGENT has the following properties:

Name Description Configurable
HTTP_PROXY Yes Sets HTTP proxy to use.
HTTPS_PROXY Yes Sets a distinct proxy to use for HTTPS requests.
NO_PROXY Yes Specifies a pattern of URLs that should be excluded from proxying. See Exclude URLs.
Supported libraries
global-agent works with all libraries that internally use http.request.

global-agent has been tested to work with:

got
axios
request
FAQ
What is the reason global-agent overrides explicitly configured HTTP(S) agent?
By default, global-agent overrides agent property of any HTTP request, even if agent property was explicitly set when constructing a HTTP request. This behaviour allows to intercept requests of libraries that use a custom instance of an agent per default (e.g. Stripe SDK uses an http(s).globalAgent instance pre-configured with keepAlive: true).

This behaviour can be disabled with GLOBAL_AGENT_FORCE_GLOBAL_AGENT=false environment variable. When disabled, then global-agent will only set agent property when it is not already defined or if agent is an instance of http(s).globalAgent.

What is the reason global-agent/bootstrap does not use HTTP_PROXY?
Some libraries (e.g. request) change their behaviour when HTTP_PROXY environment variable is present. Using a namespaced environment variable prevents conflicting library behaviour.

You can override this behaviour by configuring GLOBAL_AGENT_ENVIRONMENT_VARIABLE_NAMESPACE variable, e.g.

$ export GLOBAL_AGENT_ENVIRONMENT_VARIABLE_NAMESPACE=

Now script initialized using global-agent/bootstrap will use HTTP_PROXY, HTTPS_PROXY and NO_PROXY environment variables.

What is the difference from global-tunnel and tunnel?
global-tunnel (including global-tunnel-ng and tunnel) are designed to support legacy Node.js versions. They use various workarounds and rely on monkey-patching http.request, http.get, https.request and https.get methods.

In contrast, global-agent supports Node.js v10 and above, and does not implements workarounds for the older Node.js versions.


Smoke.

emoji nature:dash smoke
NPM version Build Status Node version XO code style License

Simple yet powerful file-based mock server with recording abilities

demo

Just drop a bunch of (JSON) files in a folder and you're ready to go!

Basic mock example
Start the server: smoke
Create a file named get_api#hello.json:
{
  "message": "hello world!"
}
Test the mock: curl http://localhost:3000/api/hello
Features
Smoke is a file-based, convention over configuration mock server that can fill your API mocking needs without any complex setup. Yet, it supports many advanced features and dynamic mocks for almost any situation:

Generate mocks quickly by recording responses from an existing server
Use folders and file names to describe API routes and REST methods
Use templates to generate responses based on input queries and route parameters
Add / edit / remove mocks without restarting the server
Generate mocks with JavaScript for more complex responses
Define different mock sets to simulate various scenarii (errors...), with fallback
Customize headers and status code if needed, automatically detect content-type if not specified
Add custom middlewares to modify requests/responses
Mock only specific requests and proxy the rest to an existing server
Supports CORS (cross-origin resource-sharing)
Installation
npm install -g smoke
Usage
See some example mocks to quickly get a grasp of the syntax and possibilities.

CLI usage is quite straightforward you can just run smoke unless you want to add some options:

Usage: smoke [<mocks_folder>] [options]

Base options:
  -p, --port <num> Server port [default: 3000]
  -h, --host <host> Server host [default: "localhost"]
  -s, --set <name> Mocks set to use [default: none]
  -n, --not-found <glob> Mocks for 404 errors [default: "404.*"]
  -i, --ignore <glob> Files to ignore [default: none]
  -k, --hooks <file> Middleware hooks [default: none]
  -x, --proxy <host> Fallback proxy if no mock found
  -o, --allow-cors [all|<hosts>] Enable CORS requests [default: none]
  --https Enable secure request serving with HTTPS [default: false]
  -l, --logs Enable server logs
  -v, --version Show version
  --help Show help

Mock recording:
  -r, --record <host> Proxy & record requests if no mock found
  -c, --collection <file> Save to single file mock collection
  -d, --depth <N> Folder depth for mocks [default: 1]
  -a, --save-headers Save response headers
  -q, --save-query Save query parameters
File naming
General format: methods_api#route#@routeParam$queryParam=value.__set.extension

The path and file name of the mock is used to determinate:

Supported HTTP methods
Optionally prefix your file by the HTTP method supported followed by an underscore (for example get_). You can specify multiple methods at once using a + to separate them (for example post+put_); If no method is specified, the mock will be used for any HTTP method.

Server route and named route parameters
Use any combination of folders or hash-separated components to specify the server route.

For example api/example/get_hello.json is equivalent to get_api#example#hello.json and will respond to GET api/example/hello requests.

Additionaly, any route component can be defined as a route parameter by prefixing the name with @, for example api#resource#@id.json will match GET api/resource/1 and expose 1 as the value for the id parameter that can be used in dynamic mocks (templates or JavaScript).

Query parameters
You can further discriminate mocks by adding query parameters to match after defining the route, using a $ (instead of the regular ?) like you would specify them in a request.

For example get_api#hello$who=john.json will match the request api/get_hello?who=john.json.

Multiple query parameters to match can be added with &, for example get_api#hello$who=john&greet=hi.json. Any specified query parameter in the file name must be matched (in any order) by the request, but the opposite is not needed.

Note that special characters must be URL-encoded, for example use get_api#hello$who=john%20doe.json to set the parameter who with the value john doe.

Tip: If you need to URL-encode a string, just run node -p "encodeURIComponent('some string')" in a terminal.

Content type
The file extension will determine the content type of the response if it's not already specified in a custom header.

Files with no extension will use the default MIME type application/octet-stream.

You can have multiple mocks with the same API route and different file extensions, the server will then use the best mock depending of the Accept header of the request.

Mock set
You can optionally specify a mock set before the file extension by using a __set-name suffix after the file name.

For example get_api#hello__error.json will only be used if you start the server with the error set enabled: smoke --set error.

If you do not specify a mock set on your file name, it will be considered as the default mock for the specified route and will be used as a fallback if no mock with this set matched.

Templates
If you add an underscore _ after the file extension, the mock will be processed as a template before being sent to the client. Templates works only on text-based formats.

For example get_hello.html_ or get_hello.json_ will be treated as templates.

Every template can use an implicit context object that have these properties defined:

method: the HTTP method of the request (ex: 'GET', 'POST')
query: map with query parameters that were part of the request URL. For example, matched URL http://server/hello?who=world will result in the query value: { who: 'world' }.
params: map containing matched route parameters. For example the mock resource#@id.json_ with the matched URL http://server/resource/123 will result in the params value: { id: '123' }.
headers: map containing request headers
body: the request body. JSON bodies are automatically parsed.
files: if the request includes multipart/form-data, this will be the array of uploaded files (see multer documentation for more details)
Template syntax
{{ }} interpolates data in place

For example, create get_hello.txt_ with this:

Hello {{query.name}}!
Then curl "http://localhost:3000/hello?name=John" returns Hello John!

{{{ }}} escapes HTML special chars from interpolated string

, create get_hello.html_ with this:

<h1>Hello {{{query.name}}}!</h1>
Then curl "http://localhost:3000/hello?name=%3CJack%26Jones%3E" returns:

<h1>Hello &lt;Jack&amp;Jones&gt;!</h1>
<{ }> evaluates JavaScript to generate data

 create get_hello.html_ with this:

Hello to:
<ul>
  <{ query.name.forEach(name => { }><li>{{name}}</li><{ }); }>
</ul>
Then curl "http://localhost:3000/hello?name=Jack&name=Jones" returns:

Hello to:
<ul>
  <li>Jack</li><li>Jones</li>
</ul>
Custom status and headers
By default all mocks responses are sent with a status code 200 (OK), or 204 (No content) if a mock file is empty.

You can customize the response status and (optionally) headers with JSON and JavaScript files, using this syntax:

{
  "statusCode": 400,
  "body": {
    "error": "Bad request"
  },
  // headers can be omitted, only use if you want to customize them
  "headers": {
    "Content-Type": "text/plain"
  } 
}
You can also use non-string content type if you encode the content as a base64 string in the body property and add the property "buffer": true to the mock:

{
  "statusCode": 200,
  "body": "U21va2Ugcm9ja3Mh",
  "buffer": true,
  "headers": {
    "Content-Type": "application/octet-stream"
  } 
}
Mock formats
Any file format is supported for mocks, and the file extension will be used to determine the response content type. Files with no extension will use the default MIME type application/octet-stream.

Text formats (for example .json, .html, .txt...) can be processed as templates by adding an underscore to the file extension.

Note that JSON files and templates must use UTF-8 encoding.

JavaScript mocks
In addition, you can define dynamic mocks using JavaScript by using the .js extension, that will be loaded as a regular Node.js module.

In that case, your JS module is expected to export a function that take an input data object with the same properties as for templates and must returns the response body or an object containing the status code, headers and body.

Example:

module.exports = (data) => `Your user agent is: ${data.headers['user-agent']}`;
Note that by default, JS mocks use application/json for the response content type. If you want to use another type, you must set the Content-Type header yourself, for example:

module.exports = data => ({
  statusCode: 200,
  headers: {
    'Content-Type': 'text/plain'
  },
  body: `Your user agent is: ${data.headers['user-agent']}`
});
Fallback proxy
If you want to override responses of an existing server, you can use the --proxy <host> option. This will proxy every request for which a mock does not exist to the specified host.

This can also be useful for mocking yet-to-be-implemented APIs and keep using real implemented APIs.

Mock recording
To quickly create a mock set of an existing server (to allow working offline for example), you can use the --record <host> option. This will proxy every request for which a mock does not exist to the specified host, and record the resulting response as a mock file.

You can change the maximum folder depth for mock files created this way using the --depth option.

The recorded mock set can also be changed using the --set option.

Instead of recoring separate mock files, you can also record to a single file mock collection using the --collection <file> option.

Note that by default response headers and request query parameters are not saved. To change this behavior, you can use the --save-headers and --save-query options.

Middleware hooks
For more advanced usages, you can hook on any standard Express middleware to modify the request and/or the response returned by the server.

To hook on your own middlewares, use the --hooks to specify a JavaScript module with exports setup like this:

module.exports = {
  before: [], // middlewares to be executed before the request is processed
  after: [] // middlewares to be executed after the request has been processed
};
Middlewares executed before the request is processed can be used to bypass regular mock response, for example to randomly simulate a server failure with an early error 500 response.

On the other hand, middlewares executed after the request have been processed can be used to augment or modify the response, for example by adding header or changing the response status. You can also access and modify the response body by using the special res.body property.

Remember that once you have used .send(), .sendStatus or .json() in a middleware the response cannot be altered anymore, that's why you should use the res.body property instead if you plan to alter the response later on.

See some example hooks.

Enabling CORS
Smoke offers support to requests originating from a different origin. However, by default, this would be disabled.

To enable CORS, pass the hosts that you want to allow to -o or --allow-cors arguments.

Accepted Values

all - Allow requests from *
<hosts> - You could also pass a comma-separated list of hosts that you want to allow requests from something like 'http://localhost:3000,http://example.com'
Single file mock collection
You can regroup multiple mocks in a special single file with the extension .mocks.js, using this format:

module.exports = {
  '<file_name>': '<file_content>' // can be a string, an object (custom response) or a function (JavaScript mock)
};
See this example mock collection to get an idea of all possibilities.

The format of file name is the same as for individual mock files, and will be used to match the request using the same rules. As for the mock content, the format is also the same as what you would put in single file mock. If a request matches both a mock file and a mock within a collection with the same specificity, the mock file will always be used over the collection.

As the format is the same, you can convert a bunch of files to a single file mock collection and conversely. To convert separate mock files to a collection:

smoke-conv <glob> <output_file> // Will create <output_file>.mocks.js from all mocks found
To convert a mock collection to separate files:

smoke-conv <file> <output_folder> // Will extract separate mocks into <output_folder>
Note that only text-based file content will be inserted directly, other file content will be converted to a base64 string.

emoji symbols:warning There is a limitation regarding JavaScript mocks: only the exported function will be converted for a given mock, meaning that if you have non-exported functions, variables or imports they will be lost during the conversion.

Other mock servers
If you cannot find what you need here, you might want to check out one of these other Node.js mock servers:

JSON Server
mockserver
node-mock-server
node-easymock
mockserver-node



Purest.

Purest
npm-version test-ci-img test-cov-img snyk-vulnerabilities

REST API Client Library

var purest = require('purest')
var google = purest({provider: 'google'})

await google
  .query('youtube')
  .select('channels')
  .where({forUsername: 'GitHub'})
  .auth(token)
  .request()
Table of Contents
This is Purest v4, for older releases take a look at v3 and v2

Introduction
Purest Options
Request Options
Examples
Article
Introduction
Purest is a tool for building expressive REST API clients

Default Endpoint
Here is a basic configuration for Google:

{
  "google": {
    "default": {
      "origin": "https://www.googleapis.com",
      "path": "{path}",
      "headers": {
        "authorization": "Bearer {auth}"
      }
    }
  }
}
The above configuration can be used to instantiate that provider:

var google = purest({provider: 'google', config})
Then we can request some data from YouTube:

var {res, body} = await google
  .get('youtube/v3/channels')
  .qs({forUsername: 'GitHub'})
  .auth(token)
  .request()
Explicit Endpoint
We can define explicit endpoint for accessing the YouTube API:

{
  "google": {
    "default": {
      "origin": "https://www.googleapis.com",
      "path": "{path}",
      "headers": {
        "authorization": "Bearer {auth}"
      }
    },
    "youtube": {
      "origin": "https://www.googleapis.com",
      "path": "youtube/{version}/{path}",
      "version": "v3",
      "headers": {
        "authorization": "Bearer {auth}"
      }
    }
  }
}
And then request the same data:

var {res, body} = await google('youtube')
  .get('channels')
  .qs({forUsername: 'GitHub'})
  .auth(token)
  .request()
Defaults
Every method in Purest can also be preconfigured with a value:

var google = purest({provider: 'google', config,
  defaults: {auth: token}
})
Then we no longer need to set the access token on each request:

var {res, body} = await google('youtube')
  .get('channels')
  .qs({forUsername: 'GitHub'})
  .request()
Method Aliases
Each method in Purest can have multiple aliases defined for it:

var google = purest({provider: 'google', config,
  defaults: {auth: token},
  methods: {get: ['select'], qs: ['where']}
})
And then use it like this:

var {res, body} = await google('youtube')
  .select('channels')
  .where({forUsername: 'GitHub'})
  .request()
Purest Options
Purest is a flexible tool for abstracting out REST APIs

var google = purest({config: {}, provider: 'google', defaults: {}, methods: {}})
| Key | Type | Description | :- | :-: | :- | provider | '' | Provider name to initialize from the list of providers found in config | config | {} | Providers configuration to use | defaults | {} | Any supported configuration option set by default, see below | methods | {} | List of methods and their aliases to use with this instance

Request Options
Purest is built on top of a powerful HTTP Client

URL Options
| Option | Description | :- | :- | origin | The protocol and domain part of the URL, can contain {subdomain} token | path | The path part of the URL, can contain {version}, {path} and {type} tokens | subdomain | Subdomain part of the URL to replace in origin | version | Version string to replace in path | type | Type string to replace in path, typically json or xml

HTTP Methods
All HTTP methods get head post put patch options delete trace connect accept a string to replace the {path} configuration token with, or absolute URL to set the entire url.

Request Options
Option Type Description
method 'string' Request method, implicitly set if one of the above HTTP Methods is used
url 'string' url object Absolute URL, automatically constructed if the URL Options above are being used, or absolute URL is passed to any of the HTTP Methods above
proxy 'string' url object Proxy URL; for HTTPS you have to use tunneling agent instead
qs {object} 'string' URL querystring
headers {object} Request headers
form {object} 'string' application/x-www-form-urlencoded request body
json {object} 'string' JSON encoded request body
multipart {object} [array] multipart/form-data as object or multipart/related as array request body using request-multipart
body 'string' Buffer Stream Raw request body
auth 'string' ['string', 'string'] {user, pass} String or array of strings to replace the {auth} configuration token with, or Basic authorization as object
oauth {object} OAuth 1.0a authorization using request-oauth
encoding 'string' Response body encoding
redirect {object} HTTP redirect configuration
timeout number Request timeout in milliseconds
agent Agent HTTP agent
Response Options
request
buffers the response body
decompresses gzip and deflate encoded bodies with valid content-encoding header
converts the response body to string using utf8 encoding by default
tries to parse JSON and querystring encoded bodies with valid content-type header
Returns either String or Object.

buffer
buffers the response body
decompresses gzip and deflate encoded bodies with valid content-encoding header
Returns Buffer.

stream
Returns the response Stream.

Node Core Options
Any other HTTP request option not explicitly exposed in Purest can be set using any of the response methods:

await google.request({socketPath: ''})
await google.buffer({socketPath: ''})
await google.stream({socketPath: ''})
Endpoint
The explicit endpoint configuration can be accessed in various ways:

// as argument to the Purest instance
await google('youtube')
// using the option name
await google.endpoint('youtube')
// or the default method alias defined for it
await google.query('youtube')
Examples
Purest comes with a fancy logger

npm i --save-dev request-logs
DEBUG=req,res,body,json node examples/file-name.js 'example name'
| Category | Topic | Providers | Example | :- | :- | :- | :- | OAuth 2.0 | Refresh Access Tokens | box google twitch | Refresh access tokens | OpenID Connect | Verify id_token | auth0 google microsoft | Discover public keys and verify id_token signature | OAuth 1.0a | OAuth 1.0a | flickr trello twitter | Get user profile | Storage | Multipart, Streams | box dropbox drive | Upload files | Storage | HTTP Streams | box dropbox | Stream file from DropBox to Box

Get access tokens using Grant.




